{"cells":[{"cell_type":"code","source":["import warnings\n","\n","# Ignore all warnings\n","warnings.filterwarnings(\"ignore\")"],"metadata":{"id":"DcsKeqbTQnon","executionInfo":{"status":"ok","timestamp":1709522336954,"user_tz":-660,"elapsed":8,"user":{"displayName":"Simon Ikoojo","userId":"12223432659917604071"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22011,"status":"ok","timestamp":1709522358959,"user":{"displayName":"Simon Ikoojo","userId":"12223432659917604071"},"user_tz":-660},"id":"DMA9H5i62C9Z","outputId":"a437c5c4-2558-4ab9-ec13-395b18e6e980"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/My Drive/Plots\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd '/content/drive/My Drive/Plots'"]},{"cell_type":"markdown","source":["#Import basic libraries"],"metadata":{"id":"uUgEpe7-eUCS"}},{"cell_type":"code","execution_count":3,"metadata":{"id":"IhAMrUt62QZD","executionInfo":{"status":"ok","timestamp":1709522360723,"user_tz":-660,"elapsed":1766,"user":{"displayName":"Simon Ikoojo","userId":"12223432659917604071"}}},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import pandas as pd\n","import numpy as np\n","import matplotlib.colors as mcolors\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score, classification_report\n","from sklearn.preprocessing import LabelEncoder\n","\n"]},{"cell_type":"markdown","source":["#Read the datasets"],"metadata":{"id":"cF-tbskFeYnn"}},{"cell_type":"code","execution_count":4,"metadata":{"id":"MItX6IAvtxPx","executionInfo":{"status":"ok","timestamp":1709522362563,"user_tz":-660,"elapsed":1842,"user":{"displayName":"Simon Ikoojo","userId":"12223432659917604071"}}},"outputs":[],"source":["\n","# Specify the file path to your CSV file\n","obrien = '/content/drive/My Drive/Plots/Obrien2022.csv'\n","vidoni = '/content/drive/My Drive/Plots/Vidoni2021.csv'\n","liu = '/content/drive/My Drive/Plots/LiuOnly2020.csv'\n","\n","# Read the CSV file into a DataFrame\n","obrien = pd.read_csv(obrien, low_memory=False)\n","vidoni = pd.read_csv(vidoni, low_memory=False)\n","liu = pd.read_csv(liu, low_memory=False)\n","# Now dfq contains the data from your CSV file\n"]},{"cell_type":"markdown","source":["#Create a simple Multinomial Naive Bayes"],"metadata":{"id":"YO3-5kf3ed87"}},{"cell_type":"code","source":[],"metadata":{"id":"oy0TpcYkgzQJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","from sklearn.naive_bayes import MultinomialNB\n","# Load the dataset\n","data = vidoni\n","\n","# Split the dataset into features (X) and target labels (y)\n","X = data['Comments']\n","y = data['TDType']\n","\n","# Text preprocessing and feature extraction using TF-IDF\n","vectorizer = TfidfVectorizer(stop_words='english')\n","X = vectorizer.fit_transform(X)\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","\n"],"metadata":{"id":"FAw-4J2yK9JR","executionInfo":{"status":"ok","timestamp":1709522363037,"user_tz":-660,"elapsed":479,"user":{"displayName":"Simon Ikoojo","userId":"12223432659917604071"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Initialize and train the classifier (e.g., Naive Bayes)\n","classifier = MultinomialNB()\n","classifier.fit(X_train, y_train)\n","\n","# Predict the labels for the test set\n","y_pred = classifier.predict(X_test)\n","\n","# Evaluate the classifier\n","print(classification_report(y_test, y_pred))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6iAt4heeOzn-","executionInfo":{"status":"ok","timestamp":1709522363037,"user_tz":-660,"elapsed":4,"user":{"displayName":"Simon Ikoojo","userId":"12223432659917604071"}},"outputId":"b5d35bf4-ffd6-4e36-adbe-917e1461de24"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["               precision    recall  f1-score   support\n","\n","    ALGORITHM       0.00      0.00      0.00        47\n"," ARCHITECTURE       1.00      0.07      0.13        57\n","        BUILD       0.00      0.00      0.00        29\n","         CODE       0.49      1.00      0.65       428\n","       DEFECT       0.83      0.07      0.13       138\n","       DESIGN       1.00      0.02      0.05        43\n","DOCUMENTATION       0.00      0.00      0.00         7\n","       PEOPLE       0.00      0.00      0.00         8\n"," REQUIREMENTS       1.00      0.05      0.09        62\n","         TEST       0.98      0.63      0.77       151\n","    USABILITY       0.00      0.00      0.00        19\n","   VERSIONING       0.00      0.00      0.00         4\n","\n","     accuracy                           0.54       993\n","    macro avg       0.44      0.15      0.15       993\n"," weighted avg       0.64      0.54      0.43       993\n","\n"]}]},{"cell_type":"markdown","source":["#Grid search CV"],"metadata":{"id":"AKZYQqQ0e1ze"}},{"cell_type":"code","source":["\n","from sklearn.model_selection import GridSearchCV\n","\n","\n","# Preprocessing: Convert text to lowercase and remove punctuation\n","data['Comments'] = data['Comments'].str.lower()\n","data['Comments'] = data['Comments'].str.replace('[^\\w\\s]', '')\n","\n","# Split the data into train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(data['Comments'], data['TDType'], test_size=0.2, random_state=42)\n","\n","# Vectorize the text data using TF-IDF vectorizer\n","tfidf_vectorizer = TfidfVectorizer()\n","X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n","X_test_tfidf = tfidf_vectorizer.transform(X_test)\n","\n","# Initialize and train the classifier (Multinomial Naive Bayes)\n","clf = MultinomialNB()\n","clf.fit(X_train_tfidf, y_train)\n","\n","# Evaluate the classifier\n","y_pred = clf.predict(X_test_tfidf)\n","print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n","print(\"Classification Report:\")\n","print(classification_report(y_test, y_pred))\n","\n","# Perform hyperparameter tuning using GridSearchCV\n","parameters = {'alpha': [0.01, 0.1, 1.0, 10.0]}\n","grid_search = GridSearchCV(clf, parameters, cv=5)\n","grid_search.fit(X_train_tfidf, y_train)\n","\n","print(\"Best parameters found:\", grid_search.best_params_)\n","\n","# Re-train the classifier with the best parameters\n","best_clf = grid_search.best_estimator_\n","best_clf.fit(X_train_tfidf, y_train)\n","\n","# Evaluate the classifier with the best parameters\n","y_pred_best = best_clf.predict(X_test_tfidf)\n","print(\"Accuracy with best parameters:\", accuracy_score(y_test, y_pred_best))\n","print(\"Classification Report with best parameters:\")\n","print(classification_report(y_test, y_pred_best))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zncVMXkDLWOP","executionInfo":{"status":"ok","timestamp":1709522363820,"user_tz":-660,"elapsed":786,"user":{"displayName":"Simon Ikoojo","userId":"12223432659917604071"}},"outputId":"53f35032-6f36-4cce-b90f-12b00337a97a"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.5246727089627392\n","Classification Report:\n","               precision    recall  f1-score   support\n","\n","    ALGORITHM       0.00      0.00      0.00        47\n"," ARCHITECTURE       1.00      0.07      0.13        57\n","        BUILD       0.00      0.00      0.00        29\n","         CODE       0.48      1.00      0.64       428\n","       DEFECT       0.86      0.04      0.08       138\n","       DESIGN       0.00      0.00      0.00        43\n","DOCUMENTATION       0.00      0.00      0.00         7\n","       PEOPLE       0.00      0.00      0.00         8\n"," REQUIREMENTS       1.00      0.03      0.06        62\n","         TEST       0.99      0.55      0.71       151\n","    USABILITY       0.00      0.00      0.00        19\n","   VERSIONING       0.00      0.00      0.00         4\n","\n","     accuracy                           0.52       993\n","    macro avg       0.36      0.14      0.14       993\n"," weighted avg       0.59      0.52      0.41       993\n","\n","Best parameters found: {'alpha': 0.1}\n","Accuracy with best parameters: 0.6122860020140987\n","Classification Report with best parameters:\n","               precision    recall  f1-score   support\n","\n","    ALGORITHM       0.57      0.09      0.15        47\n"," ARCHITECTURE       0.80      0.21      0.33        57\n","        BUILD       1.00      0.28      0.43        29\n","         CODE       0.56      0.92      0.69       428\n","       DEFECT       0.57      0.39      0.46       138\n","       DESIGN       0.83      0.12      0.20        43\n","DOCUMENTATION       0.00      0.00      0.00         7\n","       PEOPLE       0.00      0.00      0.00         8\n"," REQUIREMENTS       0.62      0.29      0.40        62\n","         TEST       0.91      0.72      0.80       151\n","    USABILITY       1.00      0.11      0.19        19\n","   VERSIONING       0.67      0.50      0.57         4\n","\n","     accuracy                           0.61       993\n","    macro avg       0.63      0.30      0.35       993\n"," weighted avg       0.66      0.61      0.56       993\n","\n"]}]},{"cell_type":"markdown","source":["#KNN Classiifer"],"metadata":{"id":"rw7A3Jc_fIZo"}},{"cell_type":"code","source":["from sklearn.neighbors import KNeighborsClassifier\n","\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Initialize the KNN classifier\n","knn = KNeighborsClassifier(n_neighbors=5)\n","\n","# Train the classifier\n","knn.fit(X_train, y_train)\n","\n","# Make predictions\n","y_pred = knn.predict(X_test)\n","\n","# Evaluate the classifier\n","print(classification_report(y_test, y_pred))\n","print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NYu4oSaCMS4v","executionInfo":{"status":"ok","timestamp":1709522364297,"user_tz":-660,"elapsed":479,"user":{"displayName":"Simon Ikoojo","userId":"12223432659917604071"}},"outputId":"f5851f74-eae1-481b-ae00-59a3f128e70f"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["               precision    recall  f1-score   support\n","\n","    ALGORITHM       0.29      0.04      0.07        47\n"," ARCHITECTURE       1.00      0.12      0.22        57\n","        BUILD       1.00      0.21      0.34        29\n","         CODE       0.48      0.98      0.65       428\n","       DEFECT       0.73      0.06      0.11       138\n","       DESIGN       0.67      0.05      0.09        43\n","DOCUMENTATION       1.00      0.14      0.25         7\n","       PEOPLE       0.00      0.00      0.00         8\n"," REQUIREMENTS       1.00      0.15      0.25        62\n","         TEST       0.97      0.49      0.65       151\n","    USABILITY       1.00      0.05      0.10        19\n","   VERSIONING       0.67      0.50      0.57         4\n","\n","     accuracy                           0.54       993\n","    macro avg       0.73      0.23      0.28       993\n"," weighted avg       0.68      0.54      0.44       993\n","\n","Accuracy: 0.5357502517623364\n"]}]},{"cell_type":"markdown","source":["## TF-IDF Implementation"],"metadata":{"id":"ile8Hpbm850P"}},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g_Z3W1uX9Xn9","executionInfo":{"status":"ok","timestamp":1709522365100,"user_tz":-660,"elapsed":804,"user":{"displayName":"Simon Ikoojo","userId":"12223432659917604071"}},"outputId":"65d86bdb-cded-40ca-98f8-608d81c5290e"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["nltk.download('stopwords')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I_zYg9VW9nnk","executionInfo":{"status":"ok","timestamp":1709522365100,"user_tz":-660,"elapsed":4,"user":{"displayName":"Simon Ikoojo","userId":"12223432659917604071"}},"outputId":"f3b39048-372f-4223-ed97-5fab617d34a1"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["import re\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import PorterStemmer\n","\n","# Load the dataset\n","data = vidoni\n","\n","# Define a function for text preprocessing\n","def preprocess_text(text):\n","    # Convert text to lowercase\n","    text = text.lower()\n","    # Remove special characters, numbers, and punctuation\n","    text = re.sub(r'[^a-z\\s]', '', text)\n","    # Tokenize the text\n","    tokens = word_tokenize(text)\n","    # Remove stopwords\n","    stop_words = set(stopwords.words('english'))\n","    tokens = [word for word in tokens if word not in stop_words]\n","    # Perform stemming\n","    stemmer = PorterStemmer()\n","    tokens = [stemmer.stem(word) for word in tokens]\n","    # Join tokens back into a string\n","    preprocessed_text = ' '.join(tokens)\n","    return preprocessed_text\n","\n","# Apply text preprocessing to the Comments column\n","data['Comments'] = data['Comments'].apply(preprocess_text)\n","\n","# Create TF-IDF vectorizer\n","tfidf_vectorizer = TfidfVectorizer()\n","\n","# Fit the vectorizer and transform the Comments column into TF-IDF vectors\n","tfidf_matrix = tfidf_vectorizer.fit_transform(data['Comments'])\n","\n","# Get the feature names (words) from the TF-IDF vectorizer\n","feature_names = tfidf_vectorizer.get_feature_names_out()\n","\n","# Create a DataFrame to store TF-IDF scores for each word\n","tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n","\n","# Add the TDType column to the DataFrame\n","tfidf_df['TDType'] = data['TDType']\n","\n","# Group by TDType and calculate the mean TF-IDF score for each word\n","tfidf_scores_by_class = tfidf_df.groupby('TDType').mean()\n","\n","# Get the top words for each class based on their TF-IDF scores\n","top_words_by_class = {}\n","for td_type, scores in tfidf_scores_by_class.iterrows():\n","    top_words_by_class[td_type] = scores.nlargest(10).index.tolist()\n","\n","# Print the top words for each class\n","for td_type, words in top_words_by_class.items():\n","    print(f\"Top words for class '{td_type}': {words}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Nw0_ijPdNefA","executionInfo":{"status":"ok","timestamp":1709522367969,"user_tz":-660,"elapsed":2871,"user":{"displayName":"Simon Ikoojo","userId":"12223432659917604071"}},"outputId":"3a0ac024-1283-4579-f93a-0bf2588775f7"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Top words for class 'ALGORITHM': ['fixm', 'todo', 'need', 'decid', 'add', 'deal', 'model', 'use', 'assum', 'data']\n","Top words for class 'ARCHITECTURE': ['slow', 'fixm', 'todo', 'memori', 'faster', 'speed', 'code', 'use', 'effici', 'make']\n","Top words for class 'BUILD': ['cmd', 'check', 'cran', 'avoid', 'note', 'packag', 'trick', 'global', 'workaround', 'hack']\n","Top words for class 'CODE': ['todo', 'fixm', 'need', 'use', 'check', 'hack', 'anymor', 'case', 'remov', 'work']\n","Top words for class 'DEFECT': ['fixm', 'work', 'fix', 'bug', 'doesnt', 'error', 'todo', 'fail', 'use', 'need']\n","Top words for class 'DESIGN': ['export', 'todo', 'function', 'object', 'class', 'fixm', 'need', 'use', 'list', 'gener']\n","Top words for class 'DOCUMENTATION': ['document', 'fixm', 'todo', 'function', 'add', 'undocu', 'develop', 'packag', 'roxygen', 'namespac']\n","Top words for class 'PEOPLE': ['fixmekelley', 'tom', 'todo', 'michel', 'michael', 'maxlen', 'temp', 'zero', 'stupid', 'clark']\n","Top words for class 'REQUIREMENTS': ['todo', 'fixm', 'add', 'data', 'want', 'select', 'handl', 'model', 'automat', 'support']\n","Top words for class 'TEST': ['nocov', 'end', 'start', 'test', 'todo', 'fail', 'add', 'check', 'fixm', 'data']\n","Top words for class 'USABILITY': ['warn', 'todo', 'user', 'messag', 'log', 'fixm', 'complain', 'option', 'inform', 'show']\n","Top words for class 'VERSIONING': ['honor', 'branch', 'default', 'todo', 'git', 'github', 'upstream', 'push', 'check', 'remot']\n"]}]},{"cell_type":"code","source":["# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Initialize the KNN classifier\n","knn = KNeighborsClassifier(n_neighbors=5)\n","\n","# Train the classifier\n","knn.fit(X_train, y_train)\n","\n","# Make predictions\n","y_pred = knn.predict(X_test)\n","\n","# Evaluate the classifier\n","print(classification_report(y_test, y_pred))\n","print(\"Accuracy:\", accuracy_score(y_test, y_pred))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tO5DsYbM9Spz","executionInfo":{"status":"ok","timestamp":1709522368432,"user_tz":-660,"elapsed":465,"user":{"displayName":"Simon Ikoojo","userId":"12223432659917604071"}},"outputId":"f9be951b-829b-4a63-e466-0d8524c0d26b"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["               precision    recall  f1-score   support\n","\n","    ALGORITHM       0.29      0.04      0.07        47\n"," ARCHITECTURE       1.00      0.12      0.22        57\n","        BUILD       1.00      0.21      0.34        29\n","         CODE       0.48      0.98      0.65       428\n","       DEFECT       0.73      0.06      0.11       138\n","       DESIGN       0.67      0.05      0.09        43\n","DOCUMENTATION       1.00      0.14      0.25         7\n","       PEOPLE       0.00      0.00      0.00         8\n"," REQUIREMENTS       1.00      0.15      0.25        62\n","         TEST       0.97      0.49      0.65       151\n","    USABILITY       1.00      0.05      0.10        19\n","   VERSIONING       0.67      0.50      0.57         4\n","\n","     accuracy                           0.54       993\n","    macro avg       0.73      0.23      0.28       993\n"," weighted avg       0.68      0.54      0.44       993\n","\n","Accuracy: 0.5357502517623364\n"]}]},{"cell_type":"markdown","source":["#FASTTEXT Implementation\n","##Partial implementation"],"metadata":{"id":"UOmYlJ38fzDs"}},{"cell_type":"code","source":["pip install fasttext"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VVKXM9Q1jGRo","executionInfo":{"status":"ok","timestamp":1709522436803,"user_tz":-660,"elapsed":68373,"user":{"displayName":"Simon Ikoojo","userId":"12223432659917604071"}},"outputId":"fcbdbf4f-61cf-4aa0-9db4-16ce6cbec1e4"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting fasttext\n","  Downloading fasttext-0.9.2.tar.gz (68 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/68.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/68.8 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.8/68.8 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting pybind11>=2.2 (from fasttext)\n","  Using cached pybind11-2.11.1-py3-none-any.whl (227 kB)\n","Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (67.7.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext) (1.25.2)\n","Building wheels for collected packages: fasttext\n","  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fasttext: filename=fasttext-0.9.2-cp310-cp310-linux_x86_64.whl size=4199774 sha256=a9ab48bdf15481c8ed43b64ea09158b17115cb9f4f17383d7e864bc89d3f82bc\n","  Stored in directory: /root/.cache/pip/wheels/a5/13/75/f811c84a8ab36eedbaef977a6a58a98990e8e0f1967f98f394\n","Successfully built fasttext\n","Installing collected packages: pybind11, fasttext\n","Successfully installed fasttext-0.9.2 pybind11-2.11.1\n"]}]},{"cell_type":"code","source":["import fasttext\n","\n","# Skipgram model :\n","model = fasttext.train_unsupervised('train.txt', model='skipgram')\n","\n","# or, cbow model :\n","model = fasttext.train_unsupervised('train.txt', model='cbow')\n"],"metadata":{"id":"34MVvTW3jOtn","executionInfo":{"status":"ok","timestamp":1709522440124,"user_tz":-660,"elapsed":3324,"user":{"displayName":"Simon Ikoojo","userId":"12223432659917604071"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["print(model.words)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8FXyvtvxPOve","executionInfo":{"status":"ok","timestamp":1709522440124,"user_tz":-660,"elapsed":6,"user":{"displayName":"Simon Ikoojo","userId":"12223432659917604071"}},"outputId":"d33591c1-5f6e-49ee-b42b-0c1d2133f1b8"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["['</s>', 'todo', 'fixm', 'use', 'need', 'nocov', 'check', 'work', 'data', 'test', 'function', 'case', 'add', 'set', 'error', 'file', 'code', 'fail', 'call', 'start', 'hack', 'make', 'remov', 'end', 'valu', 'fix', 'model', 'get', 'name', 'object', 'method', 'variabl', 'r', 'want', 'note', 'one', 'way', 'x', 'doesnt', 'time', 'handl', 'return', 'paramet', 'may', 'well', 'argument', 'column', 'better', 'dont', 'base', 'chang', 'matrix', 'could', 'line', 'first', 'also', 'list', 'avoid', 'type', 'local', 'workaround', 'na', 'warn', 'see', 'number', 'vector', 'group', 'differ', 'creat', 'like', 'new', 'user', 'sure', 'bug', 'issu', 'support', 'run', 'seem', 'trick', 'null', 'would', 'order', 'class', 'eg', 'might', 'instead', 'hope', 'allow', 'gener', 'packag', 'factor', 'encod', 'default', 'everi', 'version', 'someon', 'option', 'possibl', 'sampl', 'result', 'problem', 'utf', 'pass', 'probabl', 'select', 'think', 'true', 'miss', 'element', 'current', 'implement', 'luck', 'tri', 'done', 'cmd', 'apirequest', 'input', 'ad', 'exist', 'updat', 'next', 'assum', 'row', 'length', 'output', 'due', 'right', 'plot', 'realli', 'two', 'save', 'mean', 'yet', 'sinc', 'label', 'n', 'point', 'param', 'later', 'must', 'multipl', 'take', 'cran', 'includ', 'mayb', 'futur', 'charact', 'fals', 'thing', 'comput', 'v', 'still', 'document', 'weight', 'someth', 'stop', 'level', 'keep', 'read', 'happen', 'match', 'anymor', 'follow', 'old', 'hb', 'requir', 'defin', 'scale', 'replac', 'actual', 'global', 'alway', 'give', 'know', 'messag', 'zero', 'cant', 'format', 'break', 'tabl', 'automat', 'singl', 'find', 'els', 'ok', 'perhap', 'whether', 'block', 'otherwis', 'delet', 'specifi', 'valid', 'ignor', 'correct', 'bit', 'bad', 'deal', 'window', 'empti', 'wrong', 'c', 'slow', 'print', 'evalu', 'look', 'put', 'etc', 'fit', 'environ', 'size', 'good', 'part', 'depend', 'per', 'drop', 'l', 'varianc', 'around', 'special', 'caus', 'matric', 'index', 'calcul', 'limit', 'alreadi', 'guess', 'estim', 'numer', 'covari', 'optim', 'string', 'prior', 'inform', 'reason', 'without', 'load', 'convert', 'turn', 'id', 'correl', 'weird', 'byte', 'export', 'sort', 'ie', 'memori', 'much', 'left', 'var', 'develop', 'temporari', 'deprec', 'equal', 'posit', 'unit', 'formula', 'write', 'assign', 'never', 'intern', 'p', 'initi', 'appli', 'map', 'adjust', 'debug', 'transform', 'figur', 'rang', 'path', 'fill', 'anyway', 'copi', 'store', 'filter', 'extract', 'condit', 'back', 'report', 'decid', 'skip', 'solut', 'directli', 'target', 'constraint', 'noth', 'last', 'detect', 'header', 'move', 'separ', 'process', 'origin', 'field', 'day', 'pattern', 'standard', 'hard', 'present', 'refer', 'expect', 'cell', 'wont', 'faster', 'import', 'ugli', 'clean', 'show', 'predict', 'loop', 'provid', 'necessari', 'tag', 'modifi', 'dummi', 'directori', 'system', 'year', 'found', 'profil', 'mani', 'exampl', 'offset', 'effici', 'comment', 'given', 'complain', 'ensur', 'sometim', 'even', 'treat', 'contain', 'best', 'consid', 'stuff', 'appropri', 'beam', 'measur', 'attribut', 'dirti', 'us', 'frame', 'residu', 'count', 'metadata', 'indic', 'parallel', 'idea', 'second', 'though', 'improv', 'go', 'connect', 'let', 'complet', 'suppli', 'within', 'custom', 'full', 'avail', 'todonunoa', 'specif', 'b', 'merg', 'templat', 'mm', 'lot', 'throw', 'z', 'log', 'integ', 'slot', 'int', 'resolv', 'text', 'build', 'manual', 'step', 'arg', 'distanc', 'task', 'long', 'adcp', 'determin', 'strata', 'term', 'unless', 'constant', 'subset', 'enough', 'raster', 'exclud', 'final', 'tricki', 'entri', 'duplic', 'nice', 'compar', 'speed', 'layer', 'switch', 'prevent', 'visibl', 'normal', 'interact', 'intercept', 'color', 'space', 'insid', 'stupid', 'close', 'cluster', 'threshold', 'array', 'least', 'written', 'shouldnt', 'definit', 'altern', 'categor', 'state', 'howev', 'gene', 'bind', 'compon', 'algorithm', 'hardcod', 'restrict', 'longer', 'real', 'temp', 'there', 'occur', 'date', 'record', 'lambda', 'github', 'come', 'ever', 'compat', 'summari', 'lack', 'inf', 'less', 'invers', 'lavaan', 'grid', 'min', 'crash', 'help', 'anoth', 'filenam', 'interv', 'word', 'solv', 'produc', 'leav', 'larg', 'moment', 'anyth', 'express', 'effect', 'broken', 'extend', 'spars', 'doc', 'free', 'rescal', 'replic', 'perform', 'analysi', 'request', 'everyth', 'nolint', 'consist', 'either', 'combin', 'appear', 'via', 'ideal', 'statement', 'correctli', 'dimens', 'averag', 'abl', 'info', 'content', 'legend', 'similar', 'across', 'env', 'nb', 'link', 'quit', 'open', 'neg', 'locat', 'sens', 'functionx', 'h', 'signal', 'df', 'control', 'q', 'extra', 'smooth', 'slower', 'display', 'g', 'glob', 'variant', 'timeout', 'cach', 'structur', 'bound', 'branch', 'unus', 'w', 'col', 'api', 'item', 'explicitli', 'hacki', 'coeffici', 'forc', 'delta', 'accept', 'iter', 'event', 'node', 'cf', 'pars', 'approach', 'say', 'region', 'revers', 'doubl', 'hyperparamet', 'relat', 'stan', 'latent', 'random', 'unsign', 'behaviour', 'recurs', 'sep', 'width', 'direct', 'identifi', 'that', 'logic', 'respons', 'wait', 'stack', 'overrid', 'eleg', 'other', 'kmp', 'metric', 'remain', 'k', 'execut', 'namespac', 'kind', 'mode', 'total', 'queri', 'im', 'permut', 'peak', 'sum', 'ye', 'parent', 'key', 'graph', 'overwrit', 'distribut', 'uniqu', 'diagon', 'smarter', 'pretti', 'verbos', 'max', 'ov', 'conveni', 'echosound', 'bin', 'regress', 'search', 'partial', 'potenti', 'observ', 'addit', 'properli', 'serial', 'isnt', 'clear', 'suggest', 'upstream', 'larger', 'page', 'sequenc', 'suit', 'append', 'browser', 'expand', 'hour', 'arrow', 'non', 'axi', 'rel', 'coverag', 'trycatch', 'incorrect', 'dt', 'seri', 'pressur', 'median', 'isna', 'top', 'dumb', 'previou', 'tmp', 'repo', 'correspond', 'month', 'suffix', 'join', 'url', 'j', 'token', 'cleanup', 'rule', 'worker', 'featur', 'signatur', 'tau', 'robust', 'flag', 'easier', 'dismo', 'construct', 'problemat', 'redund', 'eml', 'rather', 'draw', 'season', 'none', 'made', 'quick', 'smaller', 'edg', 'place', 'except', 'individu', 'common', 'alter', 'tail', 'attach', 'script', 'e', 'align', 'sourc', 'side', 'henc', 'often', 'instrument', 'retent', 'simpli', 'listfil', 'instal', 'oper', 'download', 'blank', 'basic', 'seed', 'asmatrixresultsselectedvar', 'catch', 'symmetr', 'retain', 'currsign', 'ncol', 'question', 'didnt', 'decim', 'maintain', 'highli', 'style', 'choos', 'fold', 'ci', 'rewrit', 'continu', 'sigma', 'simpl', 'main', 'chunk', 'databas', 'hoc', 'colour', 'margin', 'vs', 'instanc', 'conditionalx', 'helper', 'ident', 'ggplot', 'rstudio', 'exit', 'truncat', 'confus', 'bookmark', 'cov', 'useless', 'tell', 'ive', 'sensit', 'symbol', 'imag', 'address', 'annot', 'quot', 'precis', 'densiti', 'coef', 'releas', 'nan', 'otscreatetidydata', 'imput', 'rate', 'fact', 'narm', 'datafram', 'ml', 'rep', 'age', 'tol', 'ratio', 'obj', 'raw', 'depth', 'renam', 'big', 'becom', 'therefor', 'small', 'invalid', 'float', 'whole', 'eventu', 'care', 'adapt', 'although', 'tcltk', 'mix', 'got', 'rid', 'section', 'cheat', 'maximum', 'pre', 'score', 'author', 'xc', 'togeth', 'properti', 'remot', 'altimeterstatu', 'trace', 'altimeterqu', 'storr', 'invert', 'dnabin', 'reb', 'fall', 're', 'analyt', 'edit', 'understand', 'gsw', 'unknown', 'hierarchi', 'dataset', 'reorder', 'subject', 'temperatur', 'dimnam', 'prob', 'gamma', 'bar', 'somewher', 'escap', 'lag', 'lead', 'burst', 'ask', 'getiteminfo', 'coordin', 'backward', 'involv', 'kept', 'narmtru', 'far', 'envir', 'sheet', 'descript', 'contrast', 'hessian', 'upper', 'curv', 'permit', 'fixedx', 'certain', 'orient', 'project', 'unfortun', 'march', 'hash', 'ordin', 'scheme', 'ef', 'singular', 'overlap', 'forecast', 'introduc', 'undocu', 'statist', 'ftest', 'andor', 'digit', 'beta', 'behavior', 'ff', 'parentfram', 'pvalu', 'mplu', 'past', 'nonexist', 'outsid', 'httpsgisstackexchangecomquestionsrasterbuffererrorwithpackageupd', 'f', 'suppos', 'pair', 'svd', 'seen', 'bodi', 'suppress', 'th', 'undo', 'salin', 'safe', 'sleep', 'binari', 'xxx', 'sec', 'interfac', 'hidden', 'alpha', 'purpos', 'insert', 'design', 'promis', 'taxonom', 'elimin', 'somehow', 'nest', 'saniti', 'choic', 'entir', 'cl', 'abil', 'endianlittl', 'latter', 'grow', 'om', 'vrt', 'impact', 'honor', 'account', 'segment', 'lappli', 'nrow', 'rownam', 'great', 'weve', 'eval', 'meta', 'gdb', 'xreg', 'practic', 'equat', 'center', 'internalmmpc', 'se', 'devtool', 'em', 'riski', 'titl', 'lavdata', 'userdefin', 'countri', 'corrupt', 'increas', 'interpret', 'extens', 'xlim', 'sd', 'preserv', 'argh', 'reach', 'translat', 'difficult', 'qualiti', 'vari', 'infer', 'creation', 'train', 'low', 'dynam', 'lower', 'three', 'simplifi', 'multilevel', 'deploy', 'older', 'multigroup', 'revisit', 'dll', 'high', 'outloc', 'ahr', 'divid', 'earlier', 'amplitud', 'equival', 'offici', 'jul', 'appar', 'ab', 'sex', 'polygon', 'lm', 'simul', 'independ', 'usethi', 'loglikelihood', 'push', 'juli', 'aic', 'sever', 'worth', 'theoret', 'command', 'proper', 'person', 'sign', 'nois', 'rbind', 'partabl', 'twice', 'administrativearealevel', 'ovnam', 'regardless', 'fixmegsw', 'server', 'enabl', 'lv', 'refactor', 'gradient', 'rememb', 'lh', 'fine', 'border', 'wed', 'necessarili', 'along', 'sublocalitylevel', 'session', 'reset', 'functiongi', 'wt', 'hand', 'outcom', 'action', 'convers', 'librari', 'kludg', 'flpar', 'pi', 'height', 'boundari', 'histogram', 'roxygen', 'latest', 'greater', 'slope', 'core', 'band', 'cytoband', 'simplici', 'configur', 'odd', 'fuzzi', 'aperm']\n"]}]},{"cell_type":"code","source":["print(model['Algorithm'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xg4GZQE5PnTd","executionInfo":{"status":"ok","timestamp":1709522440124,"user_tz":-660,"elapsed":5,"user":{"displayName":"Simon Ikoojo","userId":"12223432659917604071"}},"outputId":"5d37e060-95b2-4e9a-c47e-2dc54e377c1e"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["[-0.01331932 -0.02987584  0.08746994 -0.2305933   0.12873021  0.0119351\n","  0.04374043  0.0808204  -0.10769865 -0.00150669 -0.10667936  0.04789455\n"," -0.01073829  0.02297004 -0.04461262  0.08301235  0.00070967  0.09954163\n","  0.06798831 -0.06816439 -0.1051579   0.02863431 -0.11691461  0.19580486\n","  0.01560298 -0.01352675  0.1721037   0.11761338 -0.05479052  0.20277275\n"," -0.25742415  0.05998928 -0.04337452 -0.0603276  -0.10265287 -0.04107243\n"," -0.20485827 -0.17517297 -0.12700152  0.07149298  0.01205838 -0.0109686\n","  0.14568593  0.0186472  -0.13139684 -0.00788239 -0.05090308 -0.05570566\n","  0.0876175   0.04893132 -0.05922992 -0.10838698 -0.15202484  0.05487808\n"," -0.04670742 -0.16720279  0.00674873  0.22356516 -0.1562516   0.12374688\n"," -0.07978079  0.14279597  0.17816491 -0.0319544  -0.07544978 -0.02270277\n","  0.09238763  0.0376428   0.15416342 -0.19170085 -0.23447059 -0.19564824\n","  0.00139366 -0.09831797  0.10969438  0.00761874 -0.19707464  0.25467262\n","  0.07857753  0.10958096 -0.1598969  -0.13349546 -0.0503564  -0.11078288\n"," -0.06081011 -0.23399772 -0.0263048  -0.1257916   0.24284656 -0.10915618\n"," -0.12688449  0.08074296  0.17236608 -0.17675799 -0.17367974 -0.1881276\n","  0.11299045  0.18489534  0.10535339  0.00345534]\n"]}]},{"cell_type":"code","source":["import fasttext\n","\n","df = vidoni\n","\n","# Preprocess the Data\n","# 'Comments' column contains the text data and 'TDType' column contains the labels\n","preprocessed_data = df.apply(lambda row: f'__label__{row[\"TDType\"]} {row[\"Comments\"]}', axis=1)\n","\n","# Step: Save the Preprocessed Data\n","preprocessed_data.to_csv('train.txt', index=False, header=False, sep=' ')\n","\n","# Step: Train the FastText Model\n","model = fasttext.train_supervised(input='train.txt', epoch=25, lr=1.0, wordNgrams=2)\n","\n","# Step 5: Evaluate the Model\n","# Example:\n","# result = model.test(\"test.txt\")\n","# print(result.precision)\n","# print(result.recall)\n","\n","# Step 6: Use the Trained Model for Prediction\n","texts = [\"Sample text 1\", \"Sample text 2\", ...]  # List of text samples for prediction\n","labels = model.predict(texts)\n","print(labels)"],"metadata":{"id":"kFvKLSEVNvSk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Fasttext\n","###Partial"],"metadata":{"id":"Duubhp1tPZmc"}},{"cell_type":"code","source":["vidoni"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"id":"VhphI92ggo5O","executionInfo":{"status":"ok","timestamp":1709523117673,"user_tz":-660,"elapsed":315,"user":{"displayName":"Simon Ikoojo","userId":"12223432659917604071"}},"outputId":"732f57a0-960a-4dfe-c9c7-931238a2555e"},"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                 Comments  TDType\n","0                        assum curl handl    CODE\n","1                                cacheabl    CODE\n","2      impact anycpubl privat controlflag    CODE\n","3                            requir valid    CODE\n","4               todo might need put param    CODE\n","...                                   ...     ...\n","4956             todo remov function mvdl    CODE\n","4957             todo remov function mvdl    CODE\n","4958                    obviou contradict    TEST\n","4959  mip sensit larg differ valu mvdlejn    TEST\n","4960            use caus crash upgrad rxx  DEFECT\n","\n","[4961 rows x 2 columns]"],"text/html":["\n","  <div id=\"df-5ee7d4d1-f4ee-4f35-8250-47feb9e7f3c4\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Comments</th>\n","      <th>TDType</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>assum curl handl</td>\n","      <td>CODE</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>cacheabl</td>\n","      <td>CODE</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>impact anycpubl privat controlflag</td>\n","      <td>CODE</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>requir valid</td>\n","      <td>CODE</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>todo might need put param</td>\n","      <td>CODE</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>4956</th>\n","      <td>todo remov function mvdl</td>\n","      <td>CODE</td>\n","    </tr>\n","    <tr>\n","      <th>4957</th>\n","      <td>todo remov function mvdl</td>\n","      <td>CODE</td>\n","    </tr>\n","    <tr>\n","      <th>4958</th>\n","      <td>obviou contradict</td>\n","      <td>TEST</td>\n","    </tr>\n","    <tr>\n","      <th>4959</th>\n","      <td>mip sensit larg differ valu mvdlejn</td>\n","      <td>TEST</td>\n","    </tr>\n","    <tr>\n","      <th>4960</th>\n","      <td>use caus crash upgrad rxx</td>\n","      <td>DEFECT</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>4961 rows × 2 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5ee7d4d1-f4ee-4f35-8250-47feb9e7f3c4')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-5ee7d4d1-f4ee-4f35-8250-47feb9e7f3c4 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-5ee7d4d1-f4ee-4f35-8250-47feb9e7f3c4');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-facdf1b9-8aef-41c1-aa8c-bb2d1a414c4a\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-facdf1b9-8aef-41c1-aa8c-bb2d1a414c4a')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-facdf1b9-8aef-41c1-aa8c-bb2d1a414c4a button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","  <div id=\"id_9ef7c4d5-4e62-4b5d-9ed6-162d35da735a\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('vidoni')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_9ef7c4d5-4e62-4b5d-9ed6-162d35da735a button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('vidoni');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"vidoni","summary":"{\n  \"name\": \"vidoni\",\n  \"rows\": 4961,\n  \"fields\": [\n    {\n      \"column\": \"Comments\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3814,\n        \"samples\": [\n          \"fixm remov may\",\n          \"todo need valid schedul need id\",\n          \"bug somewher fixm sensit order start model matric\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"TDType\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 12,\n        \"samples\": [\n          \"USABILITY\",\n          \"VERSIONING\",\n          \"CODE\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":["import fasttext\n","from sklearn.model_selection import train_test_split\n","\n","# Load dataset\n","data = vidoni\n","\n","# Preprocess your data as needed\n","# For example, remove special characters, tokenize, lowercase, and remove stopwords\n","\n","# Split your dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(data['Comments'], data['TDType'], test_size=0.2, random_state=42)\n","\n","# Convert your data to fastText format and save to files\n","train_file = 'train.txt'\n","test_file = 'test.txt'\n","\n","with open(train_file, 'w') as f:\n","    for text, label in zip(X_train, y_train):\n","        f.write(f'__label__{label} {text}\\n')\n","\n","with open(test_file, 'w') as f:\n","    for text, label in zip(X_test, y_test):\n","        f.write(f'__label__{label} {text}\\n')\n","\n","# Train fastText model\n","model = fasttext.train_supervised(input=train_file, epoch=25, lr=0.1, wordNgrams=2, dim=100, loss='softmax')\n","\n","# Evaluate model\n","result = model.test(test_file)\n","print(f\"Precision: {result[1]:.2f}\")\n","print(f\"Recall: {result[2]:.2f}\")\n","print(f\"F1 Score: {result[1] * result[2] * 2 / (result[1] + result[2]):.2f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SivQsHsfPn8s","executionInfo":{"status":"ok","timestamp":1709522948479,"user_tz":-660,"elapsed":1672,"user":{"displayName":"Simon Ikoojo","userId":"12223432659917604071"}},"outputId":"7731077d-c479-4440-86e2-bb9cf152fe92"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Precision: 0.63\n","Recall: 0.63\n","F1 Score: 0.63\n"]}]},{"cell_type":"markdown","source":["#Binary Classification Using Random Forest"],"metadata":{"id":"wH0nUZi3hi4A"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.multioutput import MultiOutputClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder\n","from sklearn.metrics import classification_report\n","\n","# Load dataset\n","data = vidoni\n","\n","# Preprocessing\n","X = data['Comments']\n","y_multi = data['TDType']\n","y_binary = (data['TDType'] == 'ALGORITHM').astype(int)\n","\n","# Split data\n","X_train, X_test, y_multi_train, y_multi_test = train_test_split(X, y_multi, test_size=0.2, random_state=42)\n","_, _, y_binary_train, y_binary_test = train_test_split(X, y_binary, test_size=0.2, random_state=42)\n","\n","# Feature extraction\n","tfidf_vectorizer = TfidfVectorizer()\n","X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n","X_test_tfidf = tfidf_vectorizer.transform(X_test)\n","\n","# Multi-label classification\n","mlb = MultiLabelBinarizer()\n","y_multi_train_encoded = mlb.fit_transform(y_multi_train.str.split(', '))\n","y_multi_test_encoded = mlb.transform(y_multi_test.str.split(', '))\n","\n","multi_clf = MultiOutputClassifier(RandomForestClassifier())\n","multi_clf.fit(X_train_tfidf, y_multi_train_encoded)\n","multi_pred = multi_clf.predict(X_test_tfidf)\n","\n","# Binary classification for ALGORITHM detection\n","le = LabelEncoder()\n","y_binary_train_encoded = le.fit_transform(y_binary_train)\n","y_binary_test_encoded = le.transform(y_binary_test)\n","\n","binary_clf = RandomForestClassifier()\n","binary_clf.fit(X_train_tfidf, y_binary_train_encoded)\n","binary_pred = binary_clf.predict(X_test_tfidf)\n","\n","# Evaluation\n","print(\"Multi-label classification report:\\n\", classification_report(y_multi_test_encoded, multi_pred))\n","print(\"Binary classification report for ALGORITHM detection:\\n\", classification_report(y_binary_test_encoded, binary_pred))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E1Bj9SK0NwRy","executionInfo":{"status":"ok","timestamp":1709523413781,"user_tz":-660,"elapsed":17654,"user":{"displayName":"Simon Ikoojo","userId":"12223432659917604071"}},"outputId":"df642a64-3877-442a-d5a8-a44b28760136"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["Multi-label classification report:\n","               precision    recall  f1-score   support\n","\n","           0       0.67      0.13      0.21        47\n","           1       0.83      0.42      0.56        57\n","           2       1.00      0.31      0.47        29\n","           3       0.71      0.72      0.72       428\n","           4       0.76      0.30      0.43       138\n","           5       0.89      0.19      0.31        43\n","           6       1.00      0.29      0.44         7\n","           7       0.00      0.00      0.00         8\n","           8       0.93      0.23      0.36        62\n","           9       0.98      0.81      0.89       151\n","          10       1.00      0.11      0.19        19\n","          11       0.67      0.50      0.57         4\n","\n","   micro avg       0.78      0.54      0.64       993\n","   macro avg       0.79      0.33      0.43       993\n","weighted avg       0.79      0.54      0.60       993\n"," samples avg       0.54      0.54      0.54       993\n","\n","Binary classification report for ALGORITHM detection:\n","               precision    recall  f1-score   support\n","\n","           0       0.96      1.00      0.98       946\n","           1       0.60      0.13      0.21        47\n","\n","    accuracy                           0.95       993\n","   macro avg       0.78      0.56      0.59       993\n","weighted avg       0.94      0.95      0.94       993\n","\n"]}]},{"cell_type":"markdown","source":["#TF-IDF implementation --Random Forest"],"metadata":{"id":"xtPZqGXP4xKc"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score, classification_report\n","from sklearn.preprocessing import LabelEncoder\n","\n","# Load dataset\n","data = vidoni\n","\n","# Preprocessing\n","X = data['Comments']\n","y = data['TDType']\n","\n","# Label encoding\n","label_encoder = LabelEncoder()\n","y_encoded = label_encoder.fit_transform(y)\n","label_mapping = dict(zip(label_encoder.transform(label_encoder.classes_), label_encoder.classes_))\n","\n","# Feature extraction\n","tfidf_vectorizer = TfidfVectorizer()\n","X_tfidf = tfidf_vectorizer.fit_transform(X)\n","\n","# Train-test split\n","X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y_encoded, test_size=0.2, random_state=42)\n","\n","# Model training\n","clf = RandomForestClassifier()\n","clf.fit(X_train, y_train)\n","\n","# Prediction\n","y_pred = clf.predict(X_test)\n","\n","# Evaluation\n","accuracy = accuracy_score(y_test, y_pred)\n","print(\"Accuracy:\", accuracy)\n","\n","# Decode predictions and ground truth labels\n","y_pred_names = [label_mapping[label] for label in y_pred]\n","y_test_names = [label_mapping[label] for label in y_test]\n","\n","# Print classification report with TDType names\n","print(\"Classification Report:\")\n","print(classification_report(y_test_names, y_pred_names))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y7qdMuftqwkp","executionInfo":{"status":"ok","timestamp":1709523317720,"user_tz":-660,"elapsed":7678,"user":{"displayName":"Simon Ikoojo","userId":"12223432659917604071"}},"outputId":"3cbdd24b-d129-4541-bb52-d6c9c4c489e6"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.6636455186304129\n","Classification Report:\n","               precision    recall  f1-score   support\n","\n","    ALGORITHM       0.70      0.15      0.25        47\n"," ARCHITECTURE       0.87      0.35      0.50        57\n","        BUILD       1.00      0.34      0.51        29\n","         CODE       0.60      0.94      0.73       428\n","       DEFECT       0.58      0.39      0.47       138\n","       DESIGN       0.83      0.23      0.36        43\n","DOCUMENTATION       0.57      0.57      0.57         7\n","       PEOPLE       0.00      0.00      0.00         8\n"," REQUIREMENTS       0.77      0.27      0.40        62\n","         TEST       0.96      0.85      0.90       151\n","    USABILITY       1.00      0.16      0.27        19\n","   VERSIONING       0.67      0.50      0.57         4\n","\n","     accuracy                           0.66       993\n","    macro avg       0.71      0.40      0.46       993\n"," weighted avg       0.71      0.66      0.63       993\n","\n"]}]},{"cell_type":"markdown","source":["#TF-IDF--with Binary classifier"],"metadata":{"id":"teegjwzqiljJ"}},{"cell_type":"code","source":["\n","# Load dataset\n","data = vidoni\n","\n","# Preprocessing\n","X = data['Comments']\n","y = data['TDType']\n","\n","# Label encoding\n","label_encoder = LabelEncoder()\n","y_encoded = label_encoder.fit_transform(y)\n","label_mapping = dict(zip(label_encoder.transform(label_encoder.classes_), label_encoder.classes_))\n","\n","# Feature extraction\n","tfidf_vectorizer = TfidfVectorizer()\n","X_tfidf = tfidf_vectorizer.fit_transform(X)\n","\n","# Train-test split\n","X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y_encoded, test_size=0.2, random_state=42)\n","\n","# Binary classification for \"ALGORITHM\"\n","y_train_algorithm = (y_train == label_encoder.transform(['ALGORITHM'])[0])\n","y_test_algorithm = (y_test == label_encoder.transform(['ALGORITHM'])[0])\n","\n","# Model training for binary classification\n","clf_algorithm = RandomForestClassifier()\n","clf_algorithm.fit(X_train, y_train_algorithm)\n","\n","# Prediction for binary classification\n","y_pred_algorithm = clf_algorithm.predict(X_test)\n","\n","# Evaluation for binary classification\n","accuracy_algorithm = accuracy_score(y_test_algorithm, y_pred_algorithm)\n","print(\"Binary Classification (ALGORITHM) Accuracy:\", accuracy_algorithm)\n","print(classification_report(y_test_algorithm, y_pred_algorithm))\n","\n","# Model training for multi-label classification\n","clf_multilabel = RandomForestClassifier()\n","clf_multilabel.fit(X_train, y_train)\n","\n","# Prediction for multi-label classification\n","y_pred_multilabel = clf_multilabel.predict(X_test)\n","\n","# Evaluation for multi-label classification\n","accuracy_multilabel = accuracy_score(y_test, y_pred_multilabel)\n","print(\"Multi-label Classification Accuracy:\", accuracy_multilabel)\n","print(classification_report(y_test, y_pred_multilabel))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A7nAw2Oar_VF","executionInfo":{"status":"ok","timestamp":1709523652769,"user_tz":-660,"elapsed":16703,"user":{"displayName":"Simon Ikoojo","userId":"12223432659917604071"}},"outputId":"6f5f538d-8bce-407a-9e6f-b408839cefd8"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["Binary Classification (ALGORITHM) Accuracy: 0.9556898288016112\n","              precision    recall  f1-score   support\n","\n","       False       0.96      1.00      0.98       946\n","        True       0.67      0.13      0.21        47\n","\n","    accuracy                           0.96       993\n","   macro avg       0.81      0.56      0.60       993\n","weighted avg       0.94      0.96      0.94       993\n","\n","Multi-label Classification Accuracy: 0.6626384692849949\n","              precision    recall  f1-score   support\n","\n","           0       0.78      0.15      0.25        47\n","           1       0.96      0.39      0.55        57\n","           2       0.91      0.34      0.50        29\n","           3       0.60      0.94      0.73       428\n","           4       0.55      0.38      0.45       138\n","           5       0.90      0.21      0.34        43\n","           6       0.50      0.57      0.53         7\n","           7       0.00      0.00      0.00         8\n","           8       0.72      0.29      0.41        62\n","           9       0.96      0.85      0.90       151\n","          10       1.00      0.16      0.27        19\n","          11       0.67      0.50      0.57         4\n","\n","    accuracy                           0.66       993\n","   macro avg       0.71      0.40      0.46       993\n","weighted avg       0.71      0.66      0.62       993\n","\n"]}]},{"cell_type":"markdown","source":["## NTLK\n"],"metadata":{"id":"eW_DP0JWNMas"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.metrics import classification_report, confusion_matrix\n","import nltk\n","from nltk.tokenize import word_tokenize\n","import spacy\n","\n","# Download NLTK resources\n","nltk.download('punkt')\n","\n","# Load English language model for spaCy\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","# Step 1: Load the dataset\n","data = vidoni\n","\n","# Step 2: Data Preprocessing\n","# You may need to perform additional preprocessing steps here based on your data characteristics\n","\n","# Step 3: Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(data['Comments'], data['TDType'], test_size=0.2, random_state=42)\n","\n","# Step 4: Feature Extraction (TF-IDF Vectorization)\n","vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')  # You can adjust max_features as needed\n","X_train_tfidf = vectorizer.fit_transform(X_train)\n","X_test_tfidf = vectorizer.transform(X_test)\n","\n","# Step 5: Model Selection and Training\n","model = MultinomialNB()  # You can choose a different classifier based on your requirements\n","model.fit(X_train_tfidf, y_train)\n","\n","# Step 6: Model Evaluation\n","y_pred = model.predict(X_test_tfidf)\n","print(\"Classification Report:\")\n","print(classification_report(y_test, y_pred))\n","print(\"Confusion Matrix:\")\n","print(confusion_matrix(y_test, y_pred))\n","\n","# Named Entity Recognition with spaCy\n","def extract_entities(text):\n","    doc = nlp(text)\n","    entities = [(ent.text, ent.label_) for ent in doc.ents]\n","    return entities\n","\n","# Example usage\n","text = \"The company Apple Inc. is based in California.\"\n","print(\"Named Entities:\")\n","print(extract_entities(text))\n","\n","# Markov model with NLTK\n","class MarkovModel:\n","    def __init__(self):\n","        self.model = None\n","\n","    def train(self, text):\n","        tokens = word_tokenize(text)\n","        bigrams = list(nltk.bigrams(tokens))\n","        self.model = nltk.ConditionalFreqDist(bigrams)\n","\n","    def generate_text(self, seed, num_words=10):\n","        if self.model is None:\n","            raise ValueError(\"Model not trained\")\n","        word = seed\n","        text = [word]\n","        for _ in range(num_words):\n","            if word not in self.model:\n","                break\n","            word = self.model[word].max()\n","            text.append(word)\n","        return ' '.join(text)\n","\n","# Example usage\n","text = \"This is a sample text for training the Markov model.\"\n","markov_model = MarkovModel()\n","markov_model.train(text)\n","generated_text = markov_model.generate_text(\"This\")\n","print(\"Generated Text:\")\n","print(generated_text)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o82YbijENQF9","executionInfo":{"status":"ok","timestamp":1709523687126,"user_tz":-660,"elapsed":11628,"user":{"displayName":"Simon Ikoojo","userId":"12223432659917604071"}},"outputId":"e9151a9f-d5e9-4ce3-ab45-3881a0dec3cf"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Classification Report:\n","               precision    recall  f1-score   support\n","\n","    ALGORITHM       0.00      0.00      0.00        47\n"," ARCHITECTURE       1.00      0.05      0.10        57\n","        BUILD       0.00      0.00      0.00        29\n","         CODE       0.49      1.00      0.65       428\n","       DEFECT       0.82      0.07      0.12       138\n","       DESIGN       1.00      0.02      0.05        43\n","DOCUMENTATION       0.00      0.00      0.00         7\n","       PEOPLE       0.00      0.00      0.00         8\n"," REQUIREMENTS       1.00      0.05      0.09        62\n","         TEST       0.97      0.64      0.77       151\n","    USABILITY       0.00      0.00      0.00        19\n","   VERSIONING       0.00      0.00      0.00         4\n","\n","     accuracy                           0.54       993\n","    macro avg       0.44      0.15      0.15       993\n"," weighted avg       0.63      0.54      0.43       993\n","\n","Confusion Matrix:\n","[[  0   0   0  47   0   0   0   0   0   0   0   0]\n"," [  0   3   0  54   0   0   0   0   0   0   0   0]\n"," [  0   0   0  26   1   0   0   0   0   2   0   0]\n"," [  0   0   0 427   0   0   0   0   0   1   0   0]\n"," [  0   0   0 129   9   0   0   0   0   0   0   0]\n"," [  0   0   0  41   1   1   0   0   0   0   0   0]\n"," [  0   0   0   7   0   0   0   0   0   0   0   0]\n"," [  0   0   0   8   0   0   0   0   0   0   0   0]\n"," [  0   0   0  59   0   0   0   0   3   0   0   0]\n"," [  0   0   0  55   0   0   0   0   0  96   0   0]\n"," [  0   0   0  19   0   0   0   0   0   0   0   0]\n"," [  0   0   0   4   0   0   0   0   0   0   0   0]]\n","Named Entities:\n","[('Apple Inc.', 'ORG'), ('California', 'GPE')]\n","Generated Text:\n","This is a sample text for training the Markov model .\n"]}]},{"cell_type":"markdown","source":["#feature representation using Word2Vec"],"metadata":{"id":"CO-d8Y_fTMEh"}},{"cell_type":"markdown","source":["#Markov Model"],"metadata":{"id":"u4Azszf5MDc7"}},{"cell_type":"code","source":["pip install markovify"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jMBgspfyyBqV","executionInfo":{"status":"ok","timestamp":1709523764489,"user_tz":-660,"elapsed":11764,"user":{"displayName":"Simon Ikoojo","userId":"12223432659917604071"}},"outputId":"28766897-1efd-47e0-ac4f-a64bcc6bccbe"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting markovify\n","  Downloading markovify-0.9.4.tar.gz (27 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting unidecode (from markovify)\n","  Downloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: markovify\n","  Building wheel for markovify (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for markovify: filename=markovify-0.9.4-py3-none-any.whl size=18606 sha256=eb5c85967fae55646ce4665fa5cd967f442cd26b52474d8087051b5b1c376c54\n","  Stored in directory: /root/.cache/pip/wheels/ca/8c/c5/41413e24c484f883a100c63ca7b3b0362b7c6f6eb6d7c9cc7f\n","Successfully built markovify\n","Installing collected packages: unidecode, markovify\n","Successfully installed markovify-0.9.4 unidecode-1.3.8\n"]}]},{"cell_type":"markdown","source":["#Named Entity RECOG"],"metadata":{"id":"PUK59UjJ1qmh"}},{"cell_type":"code","source":["import nltk\n","nltk.download('maxent_ne_chunker')\n","nltk.download('words')\n","nltk.download('averaged_perceptron_tagger')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R-od4wBI2U7k","executionInfo":{"status":"ok","timestamp":1709523781638,"user_tz":-660,"elapsed":1051,"user":{"displayName":"Simon Ikoojo","userId":"12223432659917604071"}},"outputId":"ff1cad74-f170-447b-f908-ac1081683304"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package maxent_ne_chunker to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n","[nltk_data] Downloading package words to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/words.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":34}]},{"cell_type":"code","source":["import pandas as pd\n","\n","# Specify the file path to your CSV file\n","obrien = '/content/drive/My Drive/Plots/Obrien2022.csv'\n","vidoni = '/content/drive/My Drive/Plots/Vidoni2021.csv'\n","liu = '/content/drive/My Drive/Plots/LiuOnly2020.csv'\n","\n","# Read the CSV file into a DataFrame\n","obrien = pd.read_csv(obrien, low_memory=False)\n","vidoni = pd.read_csv(vidoni, low_memory=False)\n","liu = pd.read_csv(liu, low_memory=False)\n","# Now dfq contains the data from your CSV file\n"],"metadata":{"id":"815tLQS1KQvj","executionInfo":{"status":"ok","timestamp":1709523786802,"user_tz":-660,"elapsed":779,"user":{"displayName":"Simon Ikoojo","userId":"12223432659917604071"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.svm import SVC\n","from nltk import ne_chunk, word_tokenize, pos_tag\n","from nltk.tree import Tree\n","\n","# Load the dataset\n","data = vidoni\n","\n","# Define a function to extract named entities using NLTK's NER\n","def extract_named_entities(text):\n","    entities = []\n","    for chunk in ne_chunk(pos_tag(word_tokenize(text))):\n","        if isinstance(chunk, Tree):\n","            entities.append(\" \".join([token for token, pos in chunk.leaves()]))\n","    return \" \".join(entities)\n","\n","# Apply the function to extract named entities from the Comments column\n","data['NamedEntities'] = data['Comments'].apply(extract_named_entities)\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(data['NamedEntities'], data['TDType'], test_size=0.2, random_state=42)\n","\n","# Convert named entities into features using CountVectorizer\n","vectorizer = CountVectorizer()\n","X_train_counts = vectorizer.fit_transform(X_train)\n","X_test_counts = vectorizer.transform(X_test)\n","\n","# Train a Support Vector Classifier (SVC) model\n","model = SVC()\n","model.fit(X_train_counts, y_train)\n","\n","# Evaluate the model\n","y_pred = model.predict(X_test_counts)\n","print(\"Classification Report:\")\n","print(classification_report(y_test, y_pred))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"olajG6bU1qRJ","executionInfo":{"status":"ok","timestamp":1709523871762,"user_tz":-660,"elapsed":41689,"user":{"displayName":"Simon Ikoojo","userId":"12223432659917604071"}},"outputId":"38046497-bc6b-4474-aaea-0e62abf95634"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["Classification Report:\n","               precision    recall  f1-score   support\n","\n","    ALGORITHM       0.50      0.06      0.11        47\n"," ARCHITECTURE       1.00      0.05      0.10        57\n","        BUILD       0.57      0.14      0.22        29\n","         CODE       0.45      0.97      0.61       428\n","       DEFECT       0.47      0.07      0.11       138\n","       DESIGN       0.67      0.14      0.23        43\n","DOCUMENTATION       0.00      0.00      0.00         7\n","       PEOPLE       0.00      0.00      0.00         8\n"," REQUIREMENTS       0.27      0.05      0.08        62\n","         TEST       0.50      0.01      0.03       151\n","    USABILITY       0.00      0.00      0.00        19\n","   VERSIONING       0.00      0.00      0.00         4\n","\n","     accuracy                           0.45       993\n","    macro avg       0.37      0.12      0.13       993\n"," weighted avg       0.48      0.45      0.32       993\n","\n"]}]},{"cell_type":"code","source":["\n","from sklearn.svm import SVC\n","import spacy\n","\n","# Load the dataset\n","data = vidoni\n","\n","# Preprocess the text data\n","data['Comments'] = data['Comments'].str.lower()\n","\n","# Load the English language model for NER using spaCy\n","nlp = spacy.load('en_core_web_sm')\n","\n","# Function to extract named entities from text using spaCy\n","def extract_named_entities(text):\n","    doc = nlp(text)\n","    named_entities = [ent.text for ent in doc.ents]\n","    return ' '.join(named_entities)\n","\n","# Apply NER to extract named entities from the comments\n","data['NamedEntities'] = data['Comments'].apply(extract_named_entities)\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(data['NamedEntities'], data['TDType'], test_size=0.2, random_state=42)\n","\n","# Initialize a TF-IDF vectorizer\n","tfidf_vectorizer = TfidfVectorizer()\n","\n","# Fit and transform the training data to TF-IDF vectors\n","X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n","\n","# Transform the testing data to TF-IDF vectors\n","X_test_tfidf = tfidf_vectorizer.transform(X_test)\n","\n","# Initialize an SVM classifier\n","svm_classifier = SVC(kernel='linear')\n","\n","# Train the SVM classifier on the TF-IDF vectors\n","svm_classifier.fit(X_train_tfidf, y_train)\n","\n","# Make predictions on the testing data\n","predictions = svm_classifier.predict(X_test_tfidf)\n","\n","# Print the classification report\n","print(classification_report(y_test, predictions))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LdnYDLEEuTuO","executionInfo":{"status":"ok","timestamp":1709524009359,"user_tz":-660,"elapsed":47118,"user":{"displayName":"Simon Ikoojo","userId":"12223432659917604071"}},"outputId":"2ad49dff-17e9-414a-e16d-249d25203d0b"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["               precision    recall  f1-score   support\n","\n","    ALGORITHM       1.00      0.02      0.04        47\n"," ARCHITECTURE       0.00      0.00      0.00        57\n","        BUILD       0.00      0.00      0.00        29\n","         CODE       0.44      1.00      0.61       428\n","       DEFECT       0.64      0.05      0.09       138\n","       DESIGN       0.00      0.00      0.00        43\n","DOCUMENTATION       0.00      0.00      0.00         7\n","       PEOPLE       0.00      0.00      0.00         8\n"," REQUIREMENTS       0.00      0.00      0.00        62\n","         TEST       0.29      0.01      0.03       151\n","    USABILITY       0.00      0.00      0.00        19\n","   VERSIONING       1.00      0.25      0.40         4\n","\n","     accuracy                           0.44       993\n","    macro avg       0.28      0.11      0.10       993\n"," weighted avg       0.37      0.44      0.28       993\n","\n"]}]},{"cell_type":"code","source":["data"],"metadata":{"id":"Pf1jVi8E1wom","executionInfo":{"status":"aborted","timestamp":1709522441663,"user_tz":-660,"elapsed":7,"user":{"displayName":"Simon Ikoojo","userId":"12223432659917604071"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#unigram and bigram"],"metadata":{"id":"8_1O_CKTyEjO"}},{"cell_type":"code","source":["\n","from sklearn.model_selection import train_test_split\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.metrics import confusion_matrix, classification_report\n","\n","# Load the dataset\n","df = vidoni\n","\n","# Feature Representation\n","# Treat each comment as a document and create a feature representation\n","comments = df['Comments']\n","td_types = df['TDType']\n","\n","# Generate an alphabetical list of unigrams and count occurrence of each token\n","vectorizer = CountVectorizer()\n","X = vectorizer.fit_transform(comments)\n","\n","# Split the dataset into training, validation, and test sets\n","X_train, X_temp, y_train, y_temp = train_test_split(X, td_types, test_size=0.3, random_state=42)\n","X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n","\n","# Train the classifier\n","classifier = MultinomialNB()\n","classifier.fit(X_train, y_train)\n","\n","# Evaluate Model Performance\n","# Calculate confusion matrix for resubstitution error\n","y_pred_train = classifier.predict(X_train)\n","conf_matrix_train = confusion_matrix(y_train, y_pred_train)\n","print(\"Confusion Matrix (Training):\\n\", conf_matrix_train)\n","\n","# Evaluate the model on validation set\n","y_pred_val = classifier.predict(X_val)\n","print(\"Classification Report (Validation):\\n\", classification_report(y_val, y_pred_val))\n","\n","# Evaluate the model on test set\n","y_pred_test = classifier.predict(X_test)\n","print(\"Classification Report (Test):\\n\", classification_report(y_test, y_pred_test))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aMmlB44s1Muw","executionInfo":{"status":"ok","timestamp":1709524122044,"user_tz":-660,"elapsed":334,"user":{"displayName":"Simon Ikoojo","userId":"12223432659917604071"}},"outputId":"710f6988-fd5e-40d7-e89f-704af16d525c"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["Confusion Matrix (Training):\n"," [[  67    1    0  135    3    0    0    0    2    1    0    0]\n"," [   0  107    0   90    5    0    0    0    0    1    0    0]\n"," [   0    0   29   70    5    0    0    0    1    4    0    0]\n"," [   0    1    0 1372   11    0    0    0    2    2    0    0]\n"," [   0    0    0  128  351    0    0    0    1    3    0    0]\n"," [   0    0    0  111    6   34    0    0    2    1    0    0]\n"," [   0    0    0   29    7    0    5    0    0    1    0    0]\n"," [   0    0    0   10    2    0    0    0    0    0    0    0]\n"," [   0    0    0  133    6    0    0    0  116    1    0    0]\n"," [   0    1    0   85   12    0    0    0    0  453    0    0]\n"," [   1    0    0   41    1    0    0    0    0    2    4    0]\n"," [   0    0    0   14    1    0    0    0    0    0    0    1]]\n","Classification Report (Validation):\n","                precision    recall  f1-score   support\n","\n","    ALGORITHM       0.40      0.06      0.10        36\n"," ARCHITECTURE       0.60      0.17      0.26        36\n","        BUILD       0.67      0.08      0.14        25\n","         CODE       0.53      0.91      0.67       322\n","       DEFECT       0.55      0.34      0.42       100\n","       DESIGN       1.00      0.03      0.06        35\n","DOCUMENTATION       0.00      0.00      0.00         7\n","       PEOPLE       0.00      0.00      0.00         4\n"," REQUIREMENTS       0.50      0.16      0.24        56\n","         TEST       0.90      0.73      0.81       112\n","    USABILITY       0.00      0.00      0.00         9\n","   VERSIONING       0.00      0.00      0.00         2\n","\n","     accuracy                           0.58       744\n","    macro avg       0.43      0.21      0.22       744\n"," weighted avg       0.59      0.58      0.51       744\n","\n","Classification Report (Test):\n","                precision    recall  f1-score   support\n","\n","    ALGORITHM       0.00      0.00      0.00        31\n"," ARCHITECTURE       0.67      0.12      0.20        52\n","        BUILD       1.00      0.12      0.21        26\n","         CODE       0.53      0.91      0.67       305\n","       DEFECT       0.49      0.32      0.38       110\n","       DESIGN       0.43      0.09      0.15        32\n","DOCUMENTATION       0.00      0.00      0.00         4\n","       PEOPLE       0.00      0.00      0.00         5\n"," REQUIREMENTS       0.42      0.19      0.26        43\n","         TEST       0.89      0.79      0.83       121\n","    USABILITY       1.00      0.08      0.14        13\n","   VERSIONING       0.00      0.00      0.00         3\n","\n","     accuracy                           0.57       745\n","    macro avg       0.45      0.22      0.24       745\n"," weighted avg       0.58      0.57      0.51       745\n","\n"]}]},{"cell_type":"markdown","source":["#Word2VEc embeddings"],"metadata":{"id":"O95qp7wN4Feh"}},{"cell_type":"code","source":["# Load the dataset\n","df = vidoni\n","\n","# Feature Representation\n","# Treat each comment as a document and create a feature representation\n","comments = df['Comments']\n","td_types = df['TDType']\n","\n","# Generate an alphabetical list of unigrams and count occurrence of each token\n","vectorizer = CountVectorizer()\n","X = vectorizer.fit_transform(comments)\n","\n","# Split the dataset into training, validation, and test sets\n","X_train, X_temp, y_train, y_temp = train_test_split(X, td_types, test_size=0.3, random_state=42)\n","X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n","\n","# Train the classifier\n","classifier = MultinomialNB()\n","classifier.fit(X_train, y_train)\n","\n","# Evaluate Model Performance\n","# Calculate confusion matrix for resubstitution error\n","y_pred_train = classifier.predict(X_train)\n","conf_matrix_train = confusion_matrix(y_train, y_pred_train)\n","print(\"Confusion Matrix (Training):\\n\", conf_matrix_train)\n","\n","# Evaluate the model on validation set\n","y_pred_val = classifier.predict(X_val)\n","print(\"Classification Report (Validation):\\n\", classification_report(y_val, y_pred_val))\n","\n","# Evaluate the model on test set\n","y_pred_test = classifier.predict(X_test)\n","print(\"Classification Report (Test):\\n\", classification_report(y_test, y_pred_test))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yQ0rihgxyIUB","executionInfo":{"status":"ok","timestamp":1709524163771,"user_tz":-660,"elapsed":2,"user":{"displayName":"Simon Ikoojo","userId":"12223432659917604071"}},"outputId":"afafccb4-d6d2-46df-c0b9-ae4bc68b67bf"},"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["Confusion Matrix (Training):\n"," [[  67    1    0  135    3    0    0    0    2    1    0    0]\n"," [   0  107    0   90    5    0    0    0    0    1    0    0]\n"," [   0    0   29   70    5    0    0    0    1    4    0    0]\n"," [   0    1    0 1372   11    0    0    0    2    2    0    0]\n"," [   0    0    0  128  351    0    0    0    1    3    0    0]\n"," [   0    0    0  111    6   34    0    0    2    1    0    0]\n"," [   0    0    0   29    7    0    5    0    0    1    0    0]\n"," [   0    0    0   10    2    0    0    0    0    0    0    0]\n"," [   0    0    0  133    6    0    0    0  116    1    0    0]\n"," [   0    1    0   85   12    0    0    0    0  453    0    0]\n"," [   1    0    0   41    1    0    0    0    0    2    4    0]\n"," [   0    0    0   14    1    0    0    0    0    0    0    1]]\n","Classification Report (Validation):\n","                precision    recall  f1-score   support\n","\n","    ALGORITHM       0.40      0.06      0.10        36\n"," ARCHITECTURE       0.60      0.17      0.26        36\n","        BUILD       0.67      0.08      0.14        25\n","         CODE       0.53      0.91      0.67       322\n","       DEFECT       0.55      0.34      0.42       100\n","       DESIGN       1.00      0.03      0.06        35\n","DOCUMENTATION       0.00      0.00      0.00         7\n","       PEOPLE       0.00      0.00      0.00         4\n"," REQUIREMENTS       0.50      0.16      0.24        56\n","         TEST       0.90      0.73      0.81       112\n","    USABILITY       0.00      0.00      0.00         9\n","   VERSIONING       0.00      0.00      0.00         2\n","\n","     accuracy                           0.58       744\n","    macro avg       0.43      0.21      0.22       744\n"," weighted avg       0.59      0.58      0.51       744\n","\n","Classification Report (Test):\n","                precision    recall  f1-score   support\n","\n","    ALGORITHM       0.00      0.00      0.00        31\n"," ARCHITECTURE       0.67      0.12      0.20        52\n","        BUILD       1.00      0.12      0.21        26\n","         CODE       0.53      0.91      0.67       305\n","       DEFECT       0.49      0.32      0.38       110\n","       DESIGN       0.43      0.09      0.15        32\n","DOCUMENTATION       0.00      0.00      0.00         4\n","       PEOPLE       0.00      0.00      0.00         5\n"," REQUIREMENTS       0.42      0.19      0.26        43\n","         TEST       0.89      0.79      0.83       121\n","    USABILITY       1.00      0.08      0.14        13\n","   VERSIONING       0.00      0.00      0.00         3\n","\n","     accuracy                           0.57       745\n","    macro avg       0.45      0.22      0.24       745\n"," weighted avg       0.58      0.57      0.51       745\n","\n"]}]},{"cell_type":"markdown","source":["#WORD2VEC MODEL"],"metadata":{"id":"GQHasOCllfQb"}},{"cell_type":"code","source":["from gensim.models import Word2Vec\n","from nltk.tokenize import word_tokenize\n","\n","# Load your dataset\n","df = vidoni\n","\n","# Tokenize the comments\n","tokenized_comments = [word_tokenize(comment.lower()) for comment in df['Comments']]\n","\n","# Train Word2Vec model\n","word2vec_model = Word2Vec(sentences=tokenized_comments, vector_size=100, window=5, min_count=1, workers=4)\n","\n","# Save the trained model\n","word2vec_model.save('word2vec_model.bin')"],"metadata":{"id":"KvVpgvXw4XyT","executionInfo":{"status":"ok","timestamp":1709524247071,"user_tz":-660,"elapsed":2264,"user":{"displayName":"Simon Ikoojo","userId":"12223432659917604071"}}},"execution_count":44,"outputs":[]},{"cell_type":"code","source":["from nltk.tokenize import word_tokenize\n","from sklearn.linear_model import LogisticRegression\n","\n","\n","# Load your dataset with labeled data (Comments and TDType)\n","df = vidoni\n","\n","# Tokenize the comments\n","tokenized_comments = [word_tokenize(comment.lower()) for comment in df['Comments']]\n","\n","# Load the pre-trained Word2Vec model\n","word2vec_model = Word2Vec.load('word2vec_model.bin')\n","\n","# Generate word embeddings for each comment\n","comment_embeddings = []\n","for tokens in tokenized_comments:\n","    embeddings = [word2vec_model.wv[token] for token in tokens if token in word2vec_model.wv]\n","    if embeddings:\n","        comment_embeddings.append(sum(embeddings) / len(embeddings))\n","    else:\n","        # If no word in the comment is present in the Word2Vec vocabulary, use a zero vector\n","        comment_embeddings.append([0] * word2vec_model.vector_size)\n","\n","# Split the dataset into train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(comment_embeddings, df['TDType'], test_size=0.2, random_state=42)\n","\n","# Train a classifier (e.g., Logistic Regression)\n","classifier = LogisticRegression()\n","classifier.fit(X_train, y_train)\n","\n","# Predict on the test set\n","y_pred = classifier.predict(X_test)\n","\n","# Generate a classification report\n","report = classification_report(y_test, y_pred)\n","print(report)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pA5PDgu44HqK","executionInfo":{"status":"ok","timestamp":1709524341844,"user_tz":-660,"elapsed":1798,"user":{"displayName":"Simon Ikoojo","userId":"12223432659917604071"}},"outputId":"f11796b3-1eaf-4e3c-c946-077055660108"},"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["               precision    recall  f1-score   support\n","\n","    ALGORITHM       0.00      0.00      0.00        47\n"," ARCHITECTURE       0.00      0.00      0.00        57\n","        BUILD       0.00      0.00      0.00        29\n","         CODE       0.47      1.00      0.64       428\n","       DEFECT       0.50      0.03      0.05       138\n","       DESIGN       0.00      0.00      0.00        43\n","DOCUMENTATION       0.00      0.00      0.00         7\n","       PEOPLE       0.00      0.00      0.00         8\n"," REQUIREMENTS       0.00      0.00      0.00        62\n","         TEST       0.96      0.48      0.64       151\n","    USABILITY       0.00      0.00      0.00        19\n","   VERSIONING       0.00      0.00      0.00         4\n","\n","     accuracy                           0.51       993\n","    macro avg       0.16      0.13      0.11       993\n"," weighted avg       0.42      0.51      0.38       993\n","\n"]}]},{"cell_type":"markdown","source":["#Binary Classification"],"metadata":{"id":"N8eAeiLY6HoK"}},{"cell_type":"code","source":["import pandas as pd\n","from gensim.models import Word2Vec\n","from nltk.tokenize import word_tokenize\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score, classification_report\n","\n","# Load the dataset\n","df = vidoni\n","\n","# Preprocess the data\n","df['Comments'] = df['Comments'].apply(lambda x: x.lower())  # Convert to lowercase\n","df['Comments'] = df['Comments'].apply(lambda x: ' '.join(word_tokenize(x)))  # Tokenize\n","\n","# Define the target variable\n","df['IsAlgorithm'] = (df['TDType'] == 'ALGORITHM').astype(int)\n","\n","# Split the dataset into train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(df['Comments'], df['IsAlgorithm'], test_size=0.2, random_state=42)\n","\n","# Load the pre-trained Word2Vec model\n","word2vec_model = Word2Vec.load('word2vec_model.bin')\n","\n","# Generate word embeddings for each comment\n","def generate_embeddings(comment):\n","    tokens = word_tokenize(comment)\n","    embeddings = [word2vec_model.wv[token] for token in tokens if token in word2vec_model.wv]\n","    if embeddings:\n","        return sum(embeddings) / len(embeddings)\n","    else:\n","        return [0] * word2vec_model.vector_size\n","\n","X_train_embeddings = X_train.apply(generate_embeddings)\n","X_test_embeddings = X_test.apply(generate_embeddings)\n","\n","# Train a logistic regression classifier\n","classifier = LogisticRegression()\n","classifier.fit(list(X_train_embeddings), y_train)\n","\n","# Make predictions on the test set\n","y_pred = classifier.predict(list(X_test_embeddings))\n","\n","# Evaluate the model\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f'Accuracy: {accuracy}')\n","\n","classification_report = classification_report(y_test, y_pred)\n","print(classification_report)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UWnOdYld6KuL","executionInfo":{"status":"ok","timestamp":1709524405149,"user_tz":-660,"elapsed":2485,"user":{"displayName":"Simon Ikoojo","userId":"12223432659917604071"}},"outputId":"9c66b293-853a-47a5-f78b-56d4fecc9421"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.9526686807653575\n","              precision    recall  f1-score   support\n","\n","           0       0.95      1.00      0.98       946\n","           1       0.00      0.00      0.00        47\n","\n","    accuracy                           0.95       993\n","   macro avg       0.48      0.50      0.49       993\n","weighted avg       0.91      0.95      0.93       993\n","\n"]}]},{"cell_type":"code","source":["print(df['IsAlgorithm'].unique())"],"metadata":{"id":"KMvjAO0C6rFv","executionInfo":{"status":"aborted","timestamp":1709522441664,"user_tz":-660,"elapsed":8,"user":{"displayName":"Simon Ikoojo","userId":"12223432659917604071"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Experimenting with with  feature extraction techniques and vectorizers"],"metadata":{"id":"jQg44H0edvT1"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import classification_report\n","\n","# Load your dataset\n","\n","df = vidoni\n","\n","# Preprocess the data\n","df['Comments'] = df['Comments'].apply(lambda x: x.lower())  # Convert to lowercase\n","df['Comments'] = df['Comments'].apply(lambda x: ' '.join(word_tokenize(x)))  # Tokenize\n","\n","# Split the dataset into train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(df['Comments'], df['TDType'], test_size=0.2, random_state=42)\n","\n","# Define different vectorizers\n","vectorizers = {\n","    'CountVectorizer': CountVectorizer(),\n","    'TfidfVectorizer': TfidfVectorizer(),\n","    'HashingVectorizer': HashingVectorizer()\n","}\n","\n","# Experiment with different vectorizers and feature extraction techniques\n","for vectorizer_name, vectorizer in vectorizers.items():\n","    print(f\"Experimenting with {vectorizer_name}...\")\n","\n","    # Transform the text data using the vectorizer\n","    X_train_vectorized = vectorizer.fit_transform(X_train)\n","    X_test_vectorized = vectorizer.transform(X_test)\n","\n","    # Train a simple classifier (Logistic Regression) on the vectorized data\n","    clf = LogisticRegression(max_iter=1000)\n","    clf.fit(X_train_vectorized, y_train)\n","\n","    # Evaluate the classifier on the test set\n","    y_pred = clf.predict(X_test_vectorized)\n","\n","    # Print classification report\n","    print(f\"Classification Report for {vectorizer_name}:\")\n","    print(classification_report(y_test, y_pred))\n","    print(\"=\"*50)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JJjoOubG6N8F","executionInfo":{"status":"ok","timestamp":1709524658183,"user_tz":-660,"elapsed":237198,"user":{"displayName":"Simon Ikoojo","userId":"12223432659917604071"}},"outputId":"156f2d78-b791-449a-97d9-614fe6959ef6"},"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":["Experimenting with CountVectorizer...\n","Classification Report for CountVectorizer:\n","               precision    recall  f1-score   support\n","\n","    ALGORITHM       0.39      0.28      0.33        47\n"," ARCHITECTURE       0.69      0.39      0.49        57\n","        BUILD       0.73      0.38      0.50        29\n","         CODE       0.65      0.87      0.74       428\n","       DEFECT       0.51      0.46      0.49       138\n","       DESIGN       0.48      0.23      0.31        43\n","DOCUMENTATION       0.67      0.29      0.40         7\n","       PEOPLE       0.00      0.00      0.00         8\n"," REQUIREMENTS       0.50      0.34      0.40        62\n","         TEST       0.92      0.81      0.86       151\n","    USABILITY       0.62      0.26      0.37        19\n","   VERSIONING       0.50      0.50      0.50         4\n","\n","     accuracy                           0.65       993\n","    macro avg       0.55      0.40      0.45       993\n"," weighted avg       0.64      0.65      0.63       993\n","\n","==================================================\n","Experimenting with TfidfVectorizer...\n","Classification Report for TfidfVectorizer:\n","               precision    recall  f1-score   support\n","\n","    ALGORITHM       0.50      0.04      0.08        47\n"," ARCHITECTURE       0.80      0.21      0.33        57\n","        BUILD       1.00      0.34      0.51        29\n","         CODE       0.56      0.93      0.70       428\n","       DEFECT       0.53      0.41      0.47       138\n","       DESIGN       0.44      0.09      0.15        43\n","DOCUMENTATION       0.00      0.00      0.00         7\n","       PEOPLE       0.00      0.00      0.00         8\n"," REQUIREMENTS       0.65      0.18      0.28        62\n","         TEST       0.97      0.75      0.85       151\n","    USABILITY       0.00      0.00      0.00        19\n","   VERSIONING       0.67      0.50      0.57         4\n","\n","     accuracy                           0.61       993\n","    macro avg       0.51      0.29      0.33       993\n"," weighted avg       0.62      0.61      0.56       993\n","\n","==================================================\n","Experimenting with HashingVectorizer...\n","Classification Report for HashingVectorizer:\n","               precision    recall  f1-score   support\n","\n","    ALGORITHM       0.40      0.04      0.08        47\n"," ARCHITECTURE       0.82      0.16      0.26        57\n","        BUILD       1.00      0.21      0.34        29\n","         CODE       0.55      0.93      0.69       428\n","       DEFECT       0.53      0.41      0.46       138\n","       DESIGN       0.44      0.09      0.15        43\n","DOCUMENTATION       0.00      0.00      0.00         7\n","       PEOPLE       0.00      0.00      0.00         8\n"," REQUIREMENTS       0.69      0.18      0.28        62\n","         TEST       0.97      0.75      0.85       151\n","    USABILITY       0.00      0.00      0.00        19\n","   VERSIONING       0.00      0.00      0.00         4\n","\n","     accuracy                           0.60       993\n","    macro avg       0.45      0.23      0.26       993\n"," weighted avg       0.62      0.60      0.54       993\n","\n","==================================================\n"]}]},{"cell_type":"code","source":["pip install hmmlearn"],"metadata":{"id":"aTKDlMORQGDX","executionInfo":{"status":"aborted","timestamp":1709522441664,"user_tz":-660,"elapsed":8,"user":{"displayName":"Simon Ikoojo","userId":"12223432659917604071"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from hmmlearn import hmm\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","\n","# Step 1: Load the dataset\n","data = vidoni\n","\n","# Step 2: Preprocessing\n","# Your preprocessing code here...\n","\n","# Step 3: Feature Extraction\n","vectorizer = TfidfVectorizer(max_features=1000)\n","X = vectorizer.fit_transform(data['Comments'])\n","y = data['TDType']\n","\n","# Step 4: Model Training\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","model = hmm.GaussianHMM(n_components=10, covariance_type=\"full\", n_iter=100)\n","model.fit(X_train.toarray())\n","\n","# Step 5: Evaluation\n","y_pred = model.predict(X_test.toarray())\n","print(classification_report(y_test, y_pred))"],"metadata":{"id":"CJ2yqJr2dgGu","executionInfo":{"status":"aborted","timestamp":1709522441664,"user_tz":-660,"elapsed":8,"user":{"displayName":"Simon Ikoojo","userId":"12223432659917604071"}}},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyNTlreGdf6b+zk2hlfVfULn"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}