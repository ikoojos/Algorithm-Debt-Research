{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "\n",
        "# Ignore all warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "DcsKeqbTQnon"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMA9H5i62C9Z",
        "outputId": "05d71759-b0fe-4f3c-8897-43e674ac660a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/Plots\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd '/content/drive/My Drive/Plots'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import basic libraries"
      ],
      "metadata": {
        "id": "uUgEpe7-eUCS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "IhAMrUt62QZD"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.colors as mcolors\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Read the datasets"
      ],
      "metadata": {
        "id": "cF-tbskFeYnn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MItX6IAvtxPx"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Specify the file path to your CSV file\n",
        "obrien = '/content/drive/My Drive/Plots/Obrien2022.csv'\n",
        "vidoni = '/content/drive/My Drive/Plots/Vidoni2021.csv'\n",
        "liu = '/content/drive/My Drive/Plots/LiuOnly2020.csv'\n",
        "\n",
        "# Read the CSV file into a DataFrame\n",
        "obrien = pd.read_csv(obrien, low_memory=False)\n",
        "vidoni = pd.read_csv(vidoni, low_memory=False)\n",
        "liu = pd.read_csv(liu, low_memory=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Binary classification using SVM"
      ],
      "metadata": {
        "id": "MXfxC_pPofwa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "data = vidoni\n",
        "\n",
        "# Create binary labels\n",
        "data['BinaryLabel'] = (data['TDType'] == 'ALGORITHM').astype(int)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['Comments'], data['BinaryLabel'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert text data into numerical representations (TF-IDF)\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Train the SVM classifier\n",
        "svm_classifier = SVC(kernel='linear')\n",
        "svm_classifier.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Evaluate the classifier\n",
        "y_pred = svm_classifier.predict(X_test_tfidf)\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMO1CjAyor3e",
        "outputId": "fca9b0e9-d634-4317-82ab-2e0d842cd063"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      1.00      0.98       946\n",
            "           1       1.00      0.04      0.08        47\n",
            "\n",
            "    accuracy                           0.95       993\n",
            "   macro avg       0.98      0.52      0.53       993\n",
            "weighted avg       0.96      0.95      0.93       993\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#BINARY Classification Using RF"
      ],
      "metadata": {
        "id": "ijb6FERWpkdR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "data = liu\n",
        "\n",
        "# Preprocessing\n",
        "X = data['Comments']\n",
        "y_multi = data['TDType']\n",
        "y_binary = (data['TDType'] == 'ALGORITHM').astype(int)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_multi_train, y_multi_test = train_test_split(X, y_multi, test_size=0.2, random_state=42)\n",
        "_, _, y_binary_train, y_binary_test = train_test_split(X, y_binary, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature extraction\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "\n",
        "\n",
        "# Binary classification for ALGORITHM detection\n",
        "le = LabelEncoder()\n",
        "y_binary_train_encoded = le.fit_transform(y_binary_train)\n",
        "y_binary_test_encoded = le.transform(y_binary_test)\n",
        "\n",
        "binary_clf = RandomForestClassifier()\n",
        "binary_clf.fit(X_train_tfidf, y_binary_train_encoded)\n",
        "binary_pred = binary_clf.predict(X_test_tfidf)\n",
        "\n",
        "# Evaluation\n",
        "#print(\"Multi-label classification report:\\n\", classification_report(y_multi_test_encoded, multi_pred))\n",
        "print(\"Binary classification report for ALGORITHM detection:\\n\", classification_report(y_binary_test_encoded, binary_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpd7WZ0Vpjif",
        "outputId": "96a59a74-47eb-4abb-e193-8929654f61eb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Binary classification report for ALGORITHM detection:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      1.00     11607\n",
            "           1       0.98      0.63      0.77       290\n",
            "\n",
            "    accuracy                           0.99     11897\n",
            "   macro avg       0.99      0.82      0.88     11897\n",
            "weighted avg       0.99      0.99      0.99     11897\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "data = liu\n",
        "\n",
        "# Preprocessing\n",
        "X = data['Comments']\n",
        "y_multi = data['TDType']\n",
        "y_binary = (data['TDType'] == 'ALGORITHM').astype(int)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_multi_train, y_multi_test = train_test_split(X, y_multi, test_size=0.2, random_state=42)\n",
        "_, _, y_binary_train, y_binary_test = train_test_split(X, y_binary, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature extraction\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Binary classification for ALGORITHM detection\n",
        "le = LabelEncoder()\n",
        "y_binary_train_encoded = le.fit_transform(y_binary_train)\n",
        "y_binary_test_encoded = le.transform(y_binary_test)\n",
        "\n",
        "binary_clf = RandomForestClassifier()\n",
        "binary_clf.fit(X_train_tfidf, y_binary_train_encoded)\n",
        "binary_pred = binary_clf.predict(X_test_tfidf)\n",
        "\n",
        "# Evaluation\n",
        "#print(\"Multi-label classification report:\\n\", classification_report(y_multi_test_encoded, multi_pred))\n",
        "print(\"Binary classification report for ALGORITHM detection:\\n\", classification_report(y_binary_test_encoded, binary_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJvIuRY76QJa",
        "outputId": "70d33184-f784-412e-c09e-8c36fea23001"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Binary classification report for ALGORITHM detection:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      1.00     11607\n",
            "           1       0.98      0.63      0.77       290\n",
            "\n",
            "    accuracy                           0.99     11897\n",
            "   macro avg       0.99      0.82      0.88     11897\n",
            "weighted avg       0.99      0.99      0.99     11897\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SVM Multi class"
      ],
      "metadata": {
        "id": "jnSw_KNL_ohm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load the dataset\n",
        "data = liu\n",
        "\n",
        "# Split the dataset into features and target\n",
        "X = data['Comments']\n",
        "y = pd.get_dummies(data['TDType'])\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert text data into numerical representations (TF-IDF)\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Train the multilabel classifier (SVM)\n",
        "svm_classifier = SVC(kernel='linear')\n",
        "multilabel_classifier = MultiOutputClassifier(svm_classifier)\n",
        "multilabel_classifier.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Evaluate the classifier\n",
        "y_pred = multilabel_classifier.predict(X_test_tfidf)\n",
        "target_names = y.columns\n",
        "print(classification_report(y_test, y_pred, target_names=target_names))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBMNndlysFTW",
        "outputId": "5ffc8230-1c5c-4e48-b3ac-a9f18de5a4d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                        precision    recall  f1-score   support\n",
            "\n",
            "             ALGORITHM       0.94      0.33      0.49       290\n",
            "         COMPATIBILITY       0.96      0.49      0.65       142\n",
            "                DEFECT       0.93      0.46      0.62       213\n",
            "                DESIGN       0.88      0.88      0.88      3452\n",
            "         DOCUMENTATION       0.97      0.60      0.74        52\n",
            "        IMPLEMENTATION       0.87      0.66      0.75       604\n",
            "           MULTITHREAD       1.00      1.00      1.00         2\n",
            "                  TEST       0.83      0.68      0.75       184\n",
            "WITHOUT_CLASSIFICATION       0.97      0.98      0.98      6955\n",
            "            removeType       0.00      0.00      0.00         2\n",
            "\n",
            "             micro avg       0.94      0.90      0.92     11896\n",
            "             macro avg       0.84      0.61      0.69     11896\n",
            "          weighted avg       0.94      0.90      0.91     11896\n",
            "           samples avg       0.89      0.90      0.89     11896\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TF-IDF Implementation"
      ],
      "metadata": {
        "id": "ile8Hpbm850P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_Z3W1uX9Xn9",
        "outputId": "80458d40-b3c9-486f-94ff-ace6b5f96dcf"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_zYg9VW9nnk",
        "outputId": "72823080-e0dd-46a5-e1c5-29a39c9f3638"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Load the dataset\n",
        "data = vidoni\n",
        "\n",
        "# Define function for text preprocessing\n",
        "def preprocess_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove special characters, numbers, and punctuation\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    # Perform stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    tokens = [stemmer.stem(word) for word in tokens]\n",
        "    # Join tokens back into a string\n",
        "    preprocessed_text = ' '.join(tokens)\n",
        "    return preprocessed_text\n",
        "\n",
        "# Apply text preprocessing to the Comments column\n",
        "data['Comments'] = data['Comments'].apply(preprocess_text)\n",
        "\n",
        "# Create TF-IDF vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit the vectorizer and transform the Comments column into TF-IDF vectors\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(data['Comments'])\n",
        "\n",
        "# Get the feature names (words) from the TF-IDF vectorizer\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Create a DataFrame to store TF-IDF scores for each word\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
        "\n",
        "# Add the TDType column to the DataFrame\n",
        "tfidf_df['TDType'] = data['TDType']\n",
        "\n",
        "# Group by TDType and calculate the mean TF-IDF score for each word\n",
        "tfidf_scores_by_class = tfidf_df.groupby('TDType').mean()\n",
        "\n",
        "# Get the top words for each class based on their TF-IDF scores\n",
        "top_words_by_class = {}\n",
        "for td_type, scores in tfidf_scores_by_class.iterrows():\n",
        "    top_words_by_class[td_type] = scores.nlargest(10).index.tolist()\n",
        "\n",
        "# Print the top words for each class\n",
        "for td_type, words in top_words_by_class.items():\n",
        "    print(f\"Top words for class '{td_type}': {words}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nw0_ijPdNefA",
        "outputId": "9667cc4e-4251-4dc6-c896-0cfc7dd5bfd7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top words for class 'ALGORITHM': ['fixm', 'todo', 'need', 'decid', 'add', 'deal', 'model', 'use', 'assum', 'data']\n",
            "Top words for class 'ARCHITECTURE': ['slow', 'fixm', 'todo', 'memori', 'faster', 'speed', 'code', 'use', 'effici', 'make']\n",
            "Top words for class 'BUILD': ['cmd', 'check', 'cran', 'avoid', 'note', 'packag', 'trick', 'global', 'workaround', 'hack']\n",
            "Top words for class 'CODE': ['todo', 'fixm', 'need', 'use', 'check', 'hack', 'anymor', 'case', 'remov', 'work']\n",
            "Top words for class 'DEFECT': ['fixm', 'work', 'fix', 'bug', 'doesnt', 'error', 'todo', 'fail', 'use', 'need']\n",
            "Top words for class 'DESIGN': ['export', 'todo', 'function', 'object', 'class', 'fixm', 'need', 'use', 'list', 'gener']\n",
            "Top words for class 'DOCUMENTATION': ['document', 'fixm', 'todo', 'function', 'add', 'undocu', 'develop', 'packag', 'roxygen', 'namespac']\n",
            "Top words for class 'PEOPLE': ['fixmekelley', 'tom', 'todo', 'michel', 'michael', 'maxlen', 'temp', 'zero', 'stupid', 'clark']\n",
            "Top words for class 'REQUIREMENTS': ['todo', 'fixm', 'add', 'data', 'want', 'select', 'handl', 'model', 'automat', 'support']\n",
            "Top words for class 'TEST': ['nocov', 'end', 'start', 'test', 'todo', 'fail', 'add', 'check', 'fixm', 'data']\n",
            "Top words for class 'USABILITY': ['warn', 'todo', 'user', 'messag', 'log', 'fixm', 'complain', 'option', 'inform', 'show']\n",
            "Top words for class 'VERSIONING': ['honor', 'branch', 'default', 'todo', 'git', 'github', 'upstream', 'push', 'check', 'remot']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "data = liu\n",
        "\n",
        "# Split the dataset into features (X) and the target variable (y)\n",
        "X = data['Comments']\n",
        "y = data['TDType']\n",
        "\n",
        "# Initialize the TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer(max_features=1000)  # You can adjust max_features as needed\n",
        "\n",
        "# Convert text data into numerical TF-IDF features\n",
        "X_tfidf = vectorizer.fit_transform(X)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the SVM classifier\n",
        "svm_classifier = SVC(kernel='linear')  # You can choose different kernels based on your data\n",
        "\n",
        "# Train the classifier\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = svm_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "jS4DyBg2Brs0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#FASTTEXT Implementation\n",
        "##Partial implementation"
      ],
      "metadata": {
        "id": "UOmYlJ38fzDs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install fasttext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVKXM9Q1jGRo",
        "outputId": "a1d46376-a883-4b4d-ccb9-1219abe73c94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fasttext in /usr/local/lib/python3.10/dist-packages (0.9.2)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.10/dist-packages (from fasttext) (2.11.1)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (67.7.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext) (1.25.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fasttext\n",
        "\n",
        "# Skipgram model :\n",
        "model = fasttext.train_unsupervised('train.txt', model='skipgram')\n",
        "\n",
        "# or, cbow model :\n",
        "model = fasttext.train_unsupervised('train.txt', model='cbow')\n"
      ],
      "metadata": {
        "id": "34MVvTW3jOtn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FXyvtvxPOve",
        "outputId": "e36361a6-5687-463b-b540-cb85b76bc24b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['</s>', 'todo', 'fixm', 'use', 'need', 'nocov', 'check', 'work', 'data', 'test', 'function', 'case', 'add', 'set', 'error', 'file', 'code', 'fail', 'call', 'start', 'hack', 'make', 'remov', 'end', 'valu', 'fix', 'model', 'get', 'name', 'object', 'method', 'variabl', 'r', 'want', 'note', 'one', 'way', 'x', 'doesnt', 'time', 'handl', 'return', 'paramet', 'may', 'well', 'argument', 'column', 'better', 'dont', 'base', 'chang', 'matrix', 'could', 'line', 'first', 'also', 'list', 'avoid', 'type', 'local', 'workaround', 'na', 'warn', 'see', 'number', 'vector', 'group', 'differ', 'creat', 'like', 'new', 'user', 'sure', 'bug', 'issu', 'support', 'run', 'seem', 'trick', 'null', 'would', 'order', 'class', 'eg', 'might', 'instead', 'hope', 'allow', 'gener', 'packag', 'factor', 'encod', 'default', 'everi', 'version', 'someon', 'option', 'possibl', 'sampl', 'result', 'problem', 'utf', 'pass', 'probabl', 'select', 'think', 'true', 'miss', 'element', 'current', 'implement', 'luck', 'tri', 'done', 'cmd', 'apirequest', 'input', 'ad', 'exist', 'updat', 'next', 'assum', 'row', 'length', 'output', 'due', 'right', 'plot', 'realli', 'two', 'save', 'mean', 'yet', 'sinc', 'label', 'n', 'point', 'param', 'later', 'must', 'multipl', 'take', 'cran', 'includ', 'mayb', 'futur', 'charact', 'fals', 'thing', 'comput', 'v', 'still', 'document', 'weight', 'someth', 'stop', 'level', 'keep', 'read', 'happen', 'match', 'anymor', 'follow', 'old', 'hb', 'requir', 'defin', 'scale', 'replac', 'actual', 'global', 'alway', 'give', 'know', 'messag', 'zero', 'cant', 'format', 'break', 'tabl', 'automat', 'singl', 'find', 'els', 'ok', 'perhap', 'whether', 'block', 'otherwis', 'delet', 'specifi', 'valid', 'ignor', 'correct', 'bit', 'bad', 'deal', 'window', 'empti', 'wrong', 'c', 'slow', 'print', 'evalu', 'look', 'put', 'etc', 'fit', 'environ', 'size', 'good', 'part', 'depend', 'per', 'drop', 'l', 'varianc', 'around', 'special', 'caus', 'matric', 'index', 'calcul', 'limit', 'alreadi', 'guess', 'estim', 'numer', 'covari', 'optim', 'string', 'prior', 'inform', 'reason', 'without', 'load', 'convert', 'turn', 'id', 'correl', 'weird', 'byte', 'export', 'sort', 'ie', 'memori', 'much', 'left', 'var', 'develop', 'temporari', 'deprec', 'equal', 'posit', 'unit', 'formula', 'write', 'assign', 'never', 'intern', 'p', 'initi', 'appli', 'map', 'adjust', 'debug', 'transform', 'figur', 'rang', 'path', 'fill', 'anyway', 'copi', 'store', 'filter', 'extract', 'condit', 'back', 'report', 'decid', 'skip', 'solut', 'directli', 'target', 'constraint', 'noth', 'last', 'detect', 'header', 'move', 'separ', 'process', 'origin', 'field', 'day', 'pattern', 'standard', 'hard', 'present', 'refer', 'expect', 'cell', 'wont', 'faster', 'import', 'ugli', 'clean', 'show', 'predict', 'loop', 'provid', 'necessari', 'tag', 'modifi', 'dummi', 'directori', 'system', 'year', 'found', 'profil', 'mani', 'exampl', 'offset', 'effici', 'comment', 'given', 'complain', 'ensur', 'sometim', 'even', 'treat', 'contain', 'best', 'consid', 'stuff', 'appropri', 'beam', 'measur', 'attribut', 'dirti', 'us', 'frame', 'residu', 'count', 'metadata', 'indic', 'parallel', 'idea', 'second', 'though', 'improv', 'go', 'connect', 'let', 'complet', 'suppli', 'within', 'custom', 'full', 'avail', 'todonunoa', 'specif', 'b', 'merg', 'templat', 'mm', 'lot', 'throw', 'z', 'log', 'integ', 'slot', 'int', 'resolv', 'text', 'build', 'manual', 'step', 'arg', 'distanc', 'task', 'long', 'adcp', 'determin', 'strata', 'term', 'unless', 'constant', 'subset', 'enough', 'raster', 'exclud', 'final', 'tricki', 'entri', 'duplic', 'nice', 'compar', 'speed', 'layer', 'switch', 'prevent', 'visibl', 'normal', 'interact', 'intercept', 'color', 'space', 'insid', 'stupid', 'close', 'cluster', 'threshold', 'array', 'least', 'written', 'shouldnt', 'definit', 'altern', 'categor', 'state', 'howev', 'gene', 'bind', 'compon', 'algorithm', 'hardcod', 'restrict', 'longer', 'real', 'temp', 'there', 'occur', 'date', 'record', 'lambda', 'github', 'come', 'ever', 'compat', 'summari', 'lack', 'inf', 'less', 'invers', 'lavaan', 'grid', 'min', 'crash', 'help', 'anoth', 'filenam', 'interv', 'word', 'solv', 'produc', 'leav', 'larg', 'moment', 'anyth', 'express', 'effect', 'broken', 'extend', 'spars', 'doc', 'free', 'rescal', 'replic', 'perform', 'analysi', 'request', 'everyth', 'nolint', 'consist', 'either', 'combin', 'appear', 'via', 'ideal', 'statement', 'correctli', 'dimens', 'averag', 'abl', 'info', 'content', 'legend', 'similar', 'across', 'env', 'nb', 'link', 'quit', 'open', 'neg', 'locat', 'sens', 'functionx', 'h', 'signal', 'df', 'control', 'q', 'extra', 'smooth', 'slower', 'display', 'g', 'glob', 'variant', 'timeout', 'cach', 'structur', 'bound', 'branch', 'unus', 'w', 'col', 'api', 'item', 'explicitli', 'hacki', 'coeffici', 'forc', 'delta', 'accept', 'iter', 'event', 'node', 'cf', 'pars', 'approach', 'say', 'region', 'revers', 'doubl', 'hyperparamet', 'relat', 'stan', 'latent', 'random', 'unsign', 'behaviour', 'recurs', 'sep', 'width', 'direct', 'identifi', 'that', 'logic', 'respons', 'wait', 'stack', 'overrid', 'eleg', 'other', 'kmp', 'metric', 'remain', 'k', 'execut', 'namespac', 'kind', 'mode', 'total', 'queri', 'im', 'permut', 'peak', 'sum', 'ye', 'parent', 'key', 'graph', 'overwrit', 'distribut', 'uniqu', 'diagon', 'smarter', 'pretti', 'verbos', 'max', 'ov', 'conveni', 'echosound', 'bin', 'regress', 'search', 'partial', 'potenti', 'observ', 'addit', 'properli', 'serial', 'isnt', 'clear', 'suggest', 'upstream', 'larger', 'page', 'sequenc', 'suit', 'append', 'browser', 'expand', 'hour', 'arrow', 'non', 'axi', 'rel', 'coverag', 'trycatch', 'incorrect', 'dt', 'seri', 'pressur', 'median', 'isna', 'top', 'dumb', 'previou', 'tmp', 'repo', 'correspond', 'month', 'suffix', 'join', 'url', 'j', 'token', 'cleanup', 'rule', 'worker', 'featur', 'signatur', 'tau', 'robust', 'flag', 'easier', 'dismo', 'construct', 'problemat', 'redund', 'eml', 'rather', 'draw', 'season', 'none', 'made', 'quick', 'smaller', 'edg', 'place', 'except', 'individu', 'common', 'alter', 'tail', 'attach', 'script', 'e', 'align', 'sourc', 'side', 'henc', 'often', 'instrument', 'retent', 'simpli', 'listfil', 'instal', 'oper', 'download', 'blank', 'basic', 'seed', 'asmatrixresultsselectedvar', 'catch', 'symmetr', 'retain', 'currsign', 'ncol', 'question', 'didnt', 'decim', 'maintain', 'highli', 'style', 'choos', 'fold', 'ci', 'rewrit', 'continu', 'sigma', 'simpl', 'main', 'chunk', 'databas', 'hoc', 'colour', 'margin', 'vs', 'instanc', 'conditionalx', 'helper', 'ident', 'ggplot', 'rstudio', 'exit', 'truncat', 'confus', 'bookmark', 'cov', 'useless', 'tell', 'ive', 'sensit', 'symbol', 'imag', 'address', 'annot', 'quot', 'precis', 'densiti', 'coef', 'releas', 'nan', 'otscreatetidydata', 'imput', 'rate', 'fact', 'narm', 'datafram', 'ml', 'rep', 'age', 'tol', 'ratio', 'obj', 'raw', 'depth', 'renam', 'big', 'becom', 'therefor', 'small', 'invalid', 'float', 'whole', 'eventu', 'care', 'adapt', 'although', 'tcltk', 'mix', 'got', 'rid', 'section', 'cheat', 'maximum', 'pre', 'score', 'author', 'xc', 'togeth', 'properti', 'remot', 'altimeterstatu', 'trace', 'altimeterqu', 'storr', 'invert', 'dnabin', 'reb', 'fall', 're', 'analyt', 'edit', 'understand', 'gsw', 'unknown', 'hierarchi', 'dataset', 'reorder', 'subject', 'temperatur', 'dimnam', 'prob', 'gamma', 'bar', 'somewher', 'escap', 'lag', 'lead', 'burst', 'ask', 'getiteminfo', 'coordin', 'backward', 'involv', 'kept', 'narmtru', 'far', 'envir', 'sheet', 'descript', 'contrast', 'hessian', 'upper', 'curv', 'permit', 'fixedx', 'certain', 'orient', 'project', 'unfortun', 'march', 'hash', 'ordin', 'scheme', 'ef', 'singular', 'overlap', 'forecast', 'introduc', 'undocu', 'statist', 'ftest', 'andor', 'digit', 'beta', 'behavior', 'ff', 'parentfram', 'pvalu', 'mplu', 'past', 'nonexist', 'outsid', 'httpsgisstackexchangecomquestionsrasterbuffererrorwithpackageupd', 'f', 'suppos', 'pair', 'svd', 'seen', 'bodi', 'suppress', 'th', 'undo', 'salin', 'safe', 'sleep', 'binari', 'xxx', 'sec', 'interfac', 'hidden', 'alpha', 'purpos', 'insert', 'design', 'promis', 'taxonom', 'elimin', 'somehow', 'nest', 'saniti', 'choic', 'entir', 'cl', 'abil', 'endianlittl', 'latter', 'grow', 'om', 'vrt', 'impact', 'honor', 'account', 'segment', 'lappli', 'nrow', 'rownam', 'great', 'weve', 'eval', 'meta', 'gdb', 'xreg', 'practic', 'equat', 'center', 'internalmmpc', 'se', 'devtool', 'em', 'riski', 'titl', 'lavdata', 'userdefin', 'countri', 'corrupt', 'increas', 'interpret', 'extens', 'xlim', 'sd', 'preserv', 'argh', 'reach', 'translat', 'difficult', 'qualiti', 'vari', 'infer', 'creation', 'train', 'low', 'dynam', 'lower', 'three', 'simplifi', 'multilevel', 'deploy', 'older', 'multigroup', 'revisit', 'dll', 'high', 'outloc', 'ahr', 'divid', 'earlier', 'amplitud', 'equival', 'offici', 'jul', 'appar', 'ab', 'sex', 'polygon', 'lm', 'simul', 'independ', 'usethi', 'loglikelihood', 'push', 'juli', 'aic', 'sever', 'worth', 'theoret', 'command', 'proper', 'person', 'sign', 'nois', 'rbind', 'partabl', 'twice', 'administrativearealevel', 'ovnam', 'regardless', 'fixmegsw', 'server', 'enabl', 'lv', 'refactor', 'gradient', 'rememb', 'lh', 'fine', 'border', 'wed', 'necessarili', 'along', 'sublocalitylevel', 'session', 'reset', 'functiongi', 'wt', 'hand', 'outcom', 'action', 'convers', 'librari', 'kludg', 'flpar', 'pi', 'height', 'boundari', 'histogram', 'roxygen', 'latest', 'greater', 'slope', 'core', 'band', 'cytoband', 'simplici', 'configur', 'odd', 'fuzzi', 'aperm']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model['Algorithm'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xg4GZQE5PnTd",
        "outputId": "79b44d5a-b5ef-4af6-a7ce-b99eb971a8af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.01923659 -0.03826291  0.07134804 -0.2440327   0.15888806  0.01985367\n",
            "  0.03319588  0.10935396 -0.10906585  0.02386821 -0.09607365  0.07034907\n",
            " -0.0118741   0.03864837 -0.02850566  0.10157031 -0.01747714  0.08010167\n",
            "  0.07347971 -0.05447319 -0.09997132  0.02567375 -0.11691757  0.1907487\n",
            "  0.03195964 -0.01426449  0.1694358   0.11902526 -0.08254462  0.19932416\n",
            " -0.25532827  0.06703003 -0.0416765  -0.07396021 -0.09468097 -0.02955039\n",
            " -0.20449331 -0.1950141  -0.13525121  0.07257954 -0.01515579 -0.01683534\n",
            "  0.14189318 -0.00722726 -0.09954365 -0.0170407  -0.04882339 -0.03033301\n",
            "  0.09874701  0.05149665 -0.06765055 -0.10636277 -0.12639472  0.05281369\n",
            " -0.0588594  -0.1857572   0.01220525  0.20289919 -0.16275075  0.11156417\n",
            " -0.0794157   0.14789799  0.17793624 -0.01543019 -0.08167998 -0.00454213\n",
            "  0.08183704  0.0386159   0.12210261 -0.19682345 -0.24768063 -0.1901629\n",
            " -0.02237261 -0.08677626  0.11934367  0.00183974 -0.19299315  0.25221816\n",
            "  0.08926781  0.12072149 -0.16266057 -0.13661209 -0.05229457 -0.07829277\n",
            " -0.07650702 -0.2487278  -0.02200059 -0.10403933  0.24306825 -0.09912833\n",
            " -0.12885031  0.05847872  0.16364357 -0.1754239  -0.16941726 -0.17989495\n",
            "  0.08355448  0.17075634  0.11334175  0.03681786]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Fasttext\n",
        "###Partial"
      ],
      "metadata": {
        "id": "Duubhp1tPZmc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import fasttext\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "data = vidoni\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['Comments'], data['TDType'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert your data to fastText format and save to files\n",
        "train_file = 'train.txt'\n",
        "test_file = 'test.txt'\n",
        "\n",
        "with open(train_file, 'w') as f:\n",
        "    for text, label in zip(X_train, y_train):\n",
        "        f.write(f'__label__{label} {text}\\n')\n",
        "\n",
        "with open(test_file, 'w') as f:\n",
        "    for text, label in zip(X_test, y_test):\n",
        "        f.write(f'__label__{label} {text}\\n')\n",
        "\n",
        "# Train fastText model\n",
        "model = fasttext.train_supervised(input=train_file, epoch=25, lr=0.1, wordNgrams=2, dim=100, loss='softmax')\n",
        "\n",
        "# Evaluate model\n",
        "result = model.test(test_file)\n",
        "print(f\"Precision: {result[1]:.2f}\")\n",
        "print(f\"Recall: {result[2]:.2f}\")\n",
        "print(f\"F1 Score: {result[1] * result[2] * 2 / (result[1] + result[2]):.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SivQsHsfPn8s",
        "outputId": "c416615e-a554-4374-9d47-c2312d522ba8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.63\n",
            "Recall: 0.63\n",
            "F1 Score: 0.63\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NTLK\n"
      ],
      "metadata": {
        "id": "eW_DP0JWNMas"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import spacy\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load English language model for spaCy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "data = liu\n",
        "\n",
        "\n",
        "\n",
        "# Step 3: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['Comments'], data['TDType'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 4: Feature Extraction (TF-IDF Vectorization)\n",
        "vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')  # You can adjust max_features as needed\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Step 5: Model Selection and Training\n",
        "model = MultinomialNB()  # You can choose a different classifier based on your requirements\n",
        "model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Step 6: Model Evaluation\n",
        "y_pred = model.predict(X_test_tfidf)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Named Entity Recognition with spaCy\n",
        "def extract_entities(text):\n",
        "    doc = nlp(text)\n",
        "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "    return entities\n",
        "\n",
        "# Example usage\n",
        "text = \"The company Apple Inc. is based in California.\"\n",
        "print(\"Named Entities:\")\n",
        "print(extract_entities(text))\n",
        "\n",
        "# Markov model with NLTK\n",
        "class MarkovModel:\n",
        "    def __init__(self):\n",
        "        self.model = None\n",
        "\n",
        "    def train(self, text):\n",
        "        tokens = word_tokenize(text)\n",
        "        bigrams = list(nltk.bigrams(tokens))\n",
        "        self.model = nltk.ConditionalFreqDist(bigrams)\n",
        "\n",
        "    def generate_text(self, seed, num_words=10):\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model not trained\")\n",
        "        word = seed\n",
        "        text = [word]\n",
        "        for _ in range(num_words):\n",
        "            if word not in self.model:\n",
        "                break\n",
        "            word = self.model[word].max()\n",
        "            text.append(word)\n",
        "        return ' '.join(text)\n",
        "\n",
        "# Example usage\n",
        "text = \"This is a sample text for training the Markov model.\"\n",
        "markov_model = MarkovModel()\n",
        "markov_model.train(text)\n",
        "generated_text = markov_model.generate_text(\"This\")\n",
        "print(\"Generated Text:\")\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o82YbijENQF9",
        "outputId": "25fe8ee6-139b-41db-9f8a-bfb26db378fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    ALGORITHM       0.00      0.00      0.00        47\n",
            " ARCHITECTURE       1.00      0.05      0.10        57\n",
            "        BUILD       0.00      0.00      0.00        29\n",
            "         CODE       0.49      1.00      0.65       428\n",
            "       DEFECT       0.82      0.07      0.12       138\n",
            "       DESIGN       1.00      0.02      0.05        43\n",
            "DOCUMENTATION       0.00      0.00      0.00         7\n",
            "       PEOPLE       0.00      0.00      0.00         8\n",
            " REQUIREMENTS       1.00      0.05      0.09        62\n",
            "         TEST       0.97      0.64      0.77       151\n",
            "    USABILITY       0.00      0.00      0.00        19\n",
            "   VERSIONING       0.00      0.00      0.00         4\n",
            "\n",
            "     accuracy                           0.54       993\n",
            "    macro avg       0.44      0.15      0.15       993\n",
            " weighted avg       0.63      0.54      0.43       993\n",
            "\n",
            "Confusion Matrix:\n",
            "[[  0   0   0  47   0   0   0   0   0   0   0   0]\n",
            " [  0   3   0  54   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0  26   1   0   0   0   0   2   0   0]\n",
            " [  0   0   0 427   0   0   0   0   0   1   0   0]\n",
            " [  0   0   0 129   9   0   0   0   0   0   0   0]\n",
            " [  0   0   0  41   1   1   0   0   0   0   0   0]\n",
            " [  0   0   0   7   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   8   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0  59   0   0   0   0   3   0   0   0]\n",
            " [  0   0   0  55   0   0   0   0   0  96   0   0]\n",
            " [  0   0   0  19   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   4   0   0   0   0   0   0   0   0]]\n",
            "Named Entities:\n",
            "[('Apple Inc.', 'ORG'), ('California', 'GPE')]\n",
            "Generated Text:\n",
            "This is a sample text for training the Markov model .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Markov Model"
      ],
      "metadata": {
        "id": "u4Azszf5MDc7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install markovify"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMBgspfyyBqV",
        "outputId": "330cee36-7057-4021-e393-2c5f3d53ef1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: markovify in /usr/local/lib/python3.10/dist-packages (0.9.4)\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.10/dist-packages (from markovify) (1.3.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Named Entity RECOG"
      ],
      "metadata": {
        "id": "PUK59UjJ1qmh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-od4wBI2U7k",
        "outputId": "660f40a5-71d0-4fbe-fca3-84f6e5449fbf"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Specify the file path to your CSV file\n",
        "obrien = '/content/drive/My Drive/Plots/Obrien2022.csv'\n",
        "vidoni = '/content/drive/My Drive/Plots/Vidoni2021.csv'\n",
        "liu = '/content/drive/My Drive/Plots/LiuOnly2020.csv'\n",
        "\n",
        "# Read the CSV file into a DataFrame\n",
        "obrien = pd.read_csv(obrien, low_memory=False)\n",
        "vidoni = pd.read_csv(vidoni, low_memory=False)\n",
        "liu = pd.read_csv(liu, low_memory=False)\n"
      ],
      "metadata": {
        "id": "815tLQS1KQvj"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from nltk import ne_chunk, word_tokenize, pos_tag\n",
        "from nltk.tree import Tree\n",
        "\n",
        "# Load the dataset\n",
        "data = liu\n",
        "\n",
        "# Define a function to extract named entities using NLTK's NER\n",
        "def extract_named_entities(text):\n",
        "    entities = []\n",
        "    for chunk in ne_chunk(pos_tag(word_tokenize(text))):\n",
        "        if isinstance(chunk, Tree):\n",
        "            entities.append(\" \".join([token for token, pos in chunk.leaves()]))\n",
        "    return \" \".join(entities)\n",
        "\n",
        "# Apply the function to extract named entities from the Comments column\n",
        "data['NamedEntities'] = data['Comments'].apply(extract_named_entities)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['NamedEntities'], data['TDType'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert named entities into features using CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_counts = vectorizer.fit_transform(X_train)\n",
        "X_test_counts = vectorizer.transform(X_test)\n",
        "\n",
        "# Train a Support Vector Classifier (SVC) model\n",
        "model = SVC()\n",
        "model.fit(X_train_counts, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = model.predict(X_test_counts)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "olajG6bU1qRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.svm import SVC\n",
        "import spacy\n",
        "\n",
        "# Load the dataset\n",
        "data = vidoni\n",
        "\n",
        "# Preprocess the text data\n",
        "data['Comments'] = data['Comments'].str.lower()\n",
        "\n",
        "# Load the English language model for NER using spaCy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Function to extract named entities from text using spaCy\n",
        "def extract_named_entities(text):\n",
        "    doc = nlp(text)\n",
        "    named_entities = [ent.text for ent in doc.ents]\n",
        "    return ' '.join(named_entities)\n",
        "\n",
        "# Apply NER to extract named entities from the comments\n",
        "data['NamedEntities'] = data['Comments'].apply(extract_named_entities)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['NamedEntities'], data['TDType'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize a TF-IDF vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the training data to TF-IDF vectors\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "\n",
        "# Transform the testing data to TF-IDF vectors\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Initialize an SVM classifier\n",
        "svm_classifier = SVC(kernel='linear')\n",
        "\n",
        "# Train the SVM classifier on the TF-IDF vectors\n",
        "svm_classifier.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "predictions = svm_classifier.predict(X_test_tfidf)\n",
        "\n",
        "# Print the classification report\n",
        "print(classification_report(y_test, predictions))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LdnYDLEEuTuO",
        "outputId": "e0242c5a-9f71-451e-81c1-0fd8845b6440"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               precision    recall  f1-score   support\n",
            "\n",
            "    ALGORITHM       1.00      0.02      0.04        47\n",
            " ARCHITECTURE       0.00      0.00      0.00        57\n",
            "        BUILD       0.00      0.00      0.00        29\n",
            "         CODE       0.44      1.00      0.61       428\n",
            "       DEFECT       0.64      0.05      0.09       138\n",
            "       DESIGN       0.00      0.00      0.00        43\n",
            "DOCUMENTATION       0.00      0.00      0.00         7\n",
            "       PEOPLE       0.00      0.00      0.00         8\n",
            " REQUIREMENTS       0.00      0.00      0.00        62\n",
            "         TEST       0.29      0.01      0.03       151\n",
            "    USABILITY       0.00      0.00      0.00        19\n",
            "   VERSIONING       1.00      0.25      0.40         4\n",
            "\n",
            "     accuracy                           0.44       993\n",
            "    macro avg       0.28      0.11      0.10       993\n",
            " weighted avg       0.37      0.44      0.28       993\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "Pf1jVi8E1wom",
        "outputId": "6ebc6ffb-bce7-4b8c-9346-3a7a050190d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               Comments  TDType  \\\n",
              "0                             # assume curl will handle    CODE   \n",
              "1                                  # is this cacheable?    CODE   \n",
              "2     # what impact should any(c(\"public\", \"private\"...    CODE   \n",
              "3                                 # requires validation    CODE   \n",
              "4     # todo might need to put some params before an...    CODE   \n",
              "...                                                 ...     ...   \n",
              "4956              ## todo: remove this function (mvdl):    CODE   \n",
              "4957          # todo: remove the function below. (mvdl)    CODE   \n",
              "4958                            # obvious contradiction    TEST   \n",
              "4959  # mip is sensitive to (very) large differences...    TEST   \n",
              "4960  # this used to cause a crash after upgrading t...  DEFECT   \n",
              "\n",
              "       NamedEntities  \n",
              "0                  #  \n",
              "1                     \n",
              "2                     \n",
              "3                     \n",
              "4                     \n",
              "...              ...  \n",
              "4956               #  \n",
              "4957               #  \n",
              "4958               #  \n",
              "4959  mip 11.08.2012  \n",
              "4960               #  \n",
              "\n",
              "[4961 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7134e5b1-7ae3-4f52-a490-ee56c1fd0535\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Comments</th>\n",
              "      <th>TDType</th>\n",
              "      <th>NamedEntities</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td># assume curl will handle</td>\n",
              "      <td>CODE</td>\n",
              "      <td>#</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td># is this cacheable?</td>\n",
              "      <td>CODE</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td># what impact should any(c(\"public\", \"private\"...</td>\n",
              "      <td>CODE</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td># requires validation</td>\n",
              "      <td>CODE</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td># todo might need to put some params before an...</td>\n",
              "      <td>CODE</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4956</th>\n",
              "      <td>## todo: remove this function (mvdl):</td>\n",
              "      <td>CODE</td>\n",
              "      <td>#</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4957</th>\n",
              "      <td># todo: remove the function below. (mvdl)</td>\n",
              "      <td>CODE</td>\n",
              "      <td>#</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4958</th>\n",
              "      <td># obvious contradiction</td>\n",
              "      <td>TEST</td>\n",
              "      <td>#</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4959</th>\n",
              "      <td># mip is sensitive to (very) large differences...</td>\n",
              "      <td>TEST</td>\n",
              "      <td>mip 11.08.2012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4960</th>\n",
              "      <td># this used to cause a crash after upgrading t...</td>\n",
              "      <td>DEFECT</td>\n",
              "      <td>#</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4961 rows  3 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7134e5b1-7ae3-4f52-a490-ee56c1fd0535')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7134e5b1-7ae3-4f52-a490-ee56c1fd0535 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7134e5b1-7ae3-4f52-a490-ee56c1fd0535');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d4da2150-67dd-4be8-8855-6708224373e9\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d4da2150-67dd-4be8-8855-6708224373e9')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d4da2150-67dd-4be8-8855-6708224373e9 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_7c217c68-2906-4cb9-9712-9f1cdc0f0766\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('vidoni')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_7c217c68-2906-4cb9-9712-9f1cdc0f0766 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('vidoni');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "vidoni",
              "summary": "{\n  \"name\": \"vidoni\",\n  \"rows\": 4961,\n  \"fields\": [\n    {\n      \"column\": \"Comments\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3949,\n        \"samples\": [\n          \"## root ## i think that everything i might want to look at ## is under this children/children node ## it looks like i need to 'find' the \\\"models.core.simulation\\\" node\",\n          \"# drop first header line ## the missing value is on a line by itself, and after that ## is footer that we will ignore for now.\",\n          \"## slow code\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"TDType\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 12,\n        \"samples\": [\n          \"USABILITY\",\n          \"VERSIONING\",\n          \"CODE\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"NamedEntities\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1205,\n        \"samples\": [\n          \"# todo # # #\",\n          \"# 87/98\",\n          \"nolint\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Word2VEc embeddings"
      ],
      "metadata": {
        "id": "O95qp7wN4Feh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "df = vidoni\n",
        "\n",
        "# Feature Representation\n",
        "# Treat each comment as a document and create a feature representation\n",
        "comments = df['Comments']\n",
        "td_types = df['TDType']\n",
        "\n",
        "# Generate an alphabetical list of unigrams and count occurrence of each token\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(comments)\n",
        "\n",
        "# Split the dataset into training, validation, and test sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, td_types, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "classifier = MultinomialNB()\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate Model Performance\n",
        "# Calculate confusion matrix for resubstitution error\n",
        "y_pred_train = classifier.predict(X_train)\n",
        "conf_matrix_train = confusion_matrix(y_train, y_pred_train)\n",
        "print(\"Confusion Matrix (Training):\\n\", conf_matrix_train)\n",
        "\n",
        "# Evaluate the model on validation set\n",
        "y_pred_val = classifier.predict(X_val)\n",
        "print(\"Classification Report (Validation):\\n\", classification_report(y_val, y_pred_val))\n",
        "\n",
        "# Evaluate the model on test set\n",
        "y_pred_test = classifier.predict(X_test)\n",
        "print(\"Classification Report (Test):\\n\", classification_report(y_test, y_pred_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQ0rihgxyIUB",
        "outputId": "166dbd88-c14f-4f92-ab17-a4ed5b0b36d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix (Training):\n",
            " [[  67    1    0  135    3    0    0    0    2    1    0    0]\n",
            " [   0  107    0   90    5    0    0    0    0    1    0    0]\n",
            " [   0    0   29   70    5    0    0    0    1    4    0    0]\n",
            " [   0    1    0 1372   11    0    0    0    2    2    0    0]\n",
            " [   0    0    0  128  351    0    0    0    1    3    0    0]\n",
            " [   0    0    0  111    6   34    0    0    2    1    0    0]\n",
            " [   0    0    0   29    7    0    5    0    0    1    0    0]\n",
            " [   0    0    0   10    2    0    0    0    0    0    0    0]\n",
            " [   0    0    0  133    6    0    0    0  116    1    0    0]\n",
            " [   0    1    0   85   12    0    0    0    0  453    0    0]\n",
            " [   1    0    0   41    1    0    0    0    0    2    4    0]\n",
            " [   0    0    0   14    1    0    0    0    0    0    0    1]]\n",
            "Classification Report (Validation):\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "    ALGORITHM       0.40      0.06      0.10        36\n",
            " ARCHITECTURE       0.60      0.17      0.26        36\n",
            "        BUILD       0.67      0.08      0.14        25\n",
            "         CODE       0.53      0.91      0.67       322\n",
            "       DEFECT       0.55      0.34      0.42       100\n",
            "       DESIGN       1.00      0.03      0.06        35\n",
            "DOCUMENTATION       0.00      0.00      0.00         7\n",
            "       PEOPLE       0.00      0.00      0.00         4\n",
            " REQUIREMENTS       0.50      0.16      0.24        56\n",
            "         TEST       0.90      0.73      0.81       112\n",
            "    USABILITY       0.00      0.00      0.00         9\n",
            "   VERSIONING       0.00      0.00      0.00         2\n",
            "\n",
            "     accuracy                           0.58       744\n",
            "    macro avg       0.43      0.21      0.22       744\n",
            " weighted avg       0.59      0.58      0.51       744\n",
            "\n",
            "Classification Report (Test):\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "    ALGORITHM       0.00      0.00      0.00        31\n",
            " ARCHITECTURE       0.67      0.12      0.20        52\n",
            "        BUILD       1.00      0.12      0.21        26\n",
            "         CODE       0.53      0.91      0.67       305\n",
            "       DEFECT       0.49      0.32      0.38       110\n",
            "       DESIGN       0.43      0.09      0.15        32\n",
            "DOCUMENTATION       0.00      0.00      0.00         4\n",
            "       PEOPLE       0.00      0.00      0.00         5\n",
            " REQUIREMENTS       0.42      0.19      0.26        43\n",
            "         TEST       0.89      0.79      0.83       121\n",
            "    USABILITY       1.00      0.08      0.14        13\n",
            "   VERSIONING       0.00      0.00      0.00         3\n",
            "\n",
            "     accuracy                           0.57       745\n",
            "    macro avg       0.45      0.22      0.24       745\n",
            " weighted avg       0.58      0.57      0.51       745\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#WORD2VEC MODEL"
      ],
      "metadata": {
        "id": "GQHasOCllfQb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "df = vidoni\n",
        "\n",
        "# Tokenize the comments\n",
        "tokenized_comments = [word_tokenize(comment.lower()) for comment in df['Comments']]\n",
        "\n",
        "# Train Word2Vec model\n",
        "word2vec_model = Word2Vec(sentences=tokenized_comments, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Save the trained model\n",
        "word2vec_model.save('word2vec_model.bin')"
      ],
      "metadata": {
        "id": "KvVpgvXw4XyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "# Load your dataset with labeled data (Comments and TDType)\n",
        "df = vidoni\n",
        "\n",
        "# Tokenize the comments\n",
        "tokenized_comments = [word_tokenize(comment.lower()) for comment in df['Comments']]\n",
        "\n",
        "# Load the pre-trained Word2Vec model\n",
        "word2vec_model = Word2Vec.load('word2vec_model.bin')\n",
        "\n",
        "# Generate word embeddings for each comment\n",
        "comment_embeddings = []\n",
        "for tokens in tokenized_comments:\n",
        "    embeddings = [word2vec_model.wv[token] for token in tokens if token in word2vec_model.wv]\n",
        "    if embeddings:\n",
        "        comment_embeddings.append(sum(embeddings) / len(embeddings))\n",
        "    else:\n",
        "        # If no word in the comment is present in the Word2Vec vocabulary, use a zero vector\n",
        "        comment_embeddings.append([0] * word2vec_model.vector_size)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(comment_embeddings, df['TDType'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a classifier (e.g., Logistic Regression)\n",
        "classifier = LogisticRegression()\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Generate a classification report\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pA5PDgu44HqK",
        "outputId": "4fc60262-12f8-4892-919d-46e27c6e9c07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               precision    recall  f1-score   support\n",
            "\n",
            "    ALGORITHM       0.00      0.00      0.00        47\n",
            " ARCHITECTURE       0.00      0.00      0.00        57\n",
            "        BUILD       0.00      0.00      0.00        29\n",
            "         CODE       0.47      1.00      0.64       428\n",
            "       DEFECT       1.00      0.01      0.01       138\n",
            "       DESIGN       0.00      0.00      0.00        43\n",
            "DOCUMENTATION       0.00      0.00      0.00         7\n",
            "       PEOPLE       0.00      0.00      0.00         8\n",
            " REQUIREMENTS       0.00      0.00      0.00        62\n",
            "         TEST       0.97      0.48      0.64       151\n",
            "    USABILITY       0.00      0.00      0.00        19\n",
            "   VERSIONING       0.00      0.00      0.00         4\n",
            "\n",
            "     accuracy                           0.50       993\n",
            "    macro avg       0.20      0.12      0.11       993\n",
            " weighted avg       0.49      0.50      0.37       993\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Binary Classification"
      ],
      "metadata": {
        "id": "N8eAeiLY6HoK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = vidoni\n",
        "\n",
        "# Preprocess the data\n",
        "df['Comments'] = df['Comments'].apply(lambda x: x.lower())  # Convert to lowercase\n",
        "df['Comments'] = df['Comments'].apply(lambda x: ' '.join(word_tokenize(x)))  # Tokenize\n",
        "\n",
        "# Define the target variable\n",
        "df['IsAlgorithm'] = (df['TDType'] == 'ALGORITHM').astype(int)\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['Comments'], df['IsAlgorithm'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Load the pre-trained Word2Vec model\n",
        "word2vec_model = Word2Vec.load('word2vec_model.bin')\n",
        "\n",
        "# Generate word embeddings for each comment\n",
        "def generate_embeddings(comment):\n",
        "    tokens = word_tokenize(comment)\n",
        "    embeddings = [word2vec_model.wv[token] for token in tokens if token in word2vec_model.wv]\n",
        "    if embeddings:\n",
        "        return sum(embeddings) / len(embeddings)\n",
        "    else:\n",
        "        return [0] * word2vec_model.vector_size\n",
        "\n",
        "X_train_embeddings = X_train.apply(generate_embeddings)\n",
        "X_test_embeddings = X_test.apply(generate_embeddings)\n",
        "\n",
        "# Train a logistic regression classifier\n",
        "classifier = LogisticRegression()\n",
        "classifier.fit(list(X_train_embeddings), y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = classifier.predict(list(X_test_embeddings))\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy}')\n",
        "\n",
        "classification_report = classification_report(y_test, y_pred)\n",
        "print(classification_report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWnOdYld6KuL",
        "outputId": "caf51df5-c461-48fb-c14f-33c4ff728343"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9526686807653575\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      1.00      0.98       946\n",
            "           1       0.00      0.00      0.00        47\n",
            "\n",
            "    accuracy                           0.95       993\n",
            "   macro avg       0.48      0.50      0.49       993\n",
            "weighted avg       0.91      0.95      0.93       993\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['IsAlgorithm'].unique())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMvjAO0C6rFv",
        "outputId": "982a518f-6864-49f5-a33d-3e89c5ccba1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Experimenting with with  feature extraction techniques and vectorizers"
      ],
      "metadata": {
        "id": "jQg44H0edvT1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load your dataset\n",
        "\n",
        "df = vidoni\n",
        "\n",
        "# Preprocess the data\n",
        "df['Comments'] = df['Comments'].apply(lambda x: x.lower())  # Convert to lowercase\n",
        "df['Comments'] = df['Comments'].apply(lambda x: ' '.join(word_tokenize(x)))  # Tokenize\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['Comments'], df['TDType'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Define different vectorizers\n",
        "vectorizers = {\n",
        "    'CountVectorizer': CountVectorizer(),\n",
        "    'TfidfVectorizer': TfidfVectorizer(),\n",
        "    'HashingVectorizer': HashingVectorizer()\n",
        "}\n",
        "\n",
        "# Experiment with different vectorizers and feature extraction techniques\n",
        "for vectorizer_name, vectorizer in vectorizers.items():\n",
        "    print(f\"Experimenting with {vectorizer_name}...\")\n",
        "\n",
        "    # Transform the text data using the vectorizer\n",
        "    X_train_vectorized = vectorizer.fit_transform(X_train)\n",
        "    X_test_vectorized = vectorizer.transform(X_test)\n",
        "\n",
        "    # Train a simple classifier (Logistic Regression) on the vectorized data\n",
        "    clf = LogisticRegression(max_iter=1000)\n",
        "    clf.fit(X_train_vectorized, y_train)\n",
        "\n",
        "    # Evaluate the classifier on the test set\n",
        "    y_pred = clf.predict(X_test_vectorized)\n",
        "\n",
        "    # Print classification report\n",
        "    print(f\"Classification Report for {vectorizer_name}:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    print(\"=\"*50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGoofSMzyimt",
        "outputId": "d41edcb3-57b0-4026-9844-a92ec403599c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experimenting with CountVectorizer...\n",
            "Classification Report for CountVectorizer:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    ALGORITHM       0.39      0.28      0.33        47\n",
            " ARCHITECTURE       0.69      0.39      0.49        57\n",
            "        BUILD       0.73      0.38      0.50        29\n",
            "         CODE       0.65      0.87      0.74       428\n",
            "       DEFECT       0.51      0.46      0.49       138\n",
            "       DESIGN       0.48      0.23      0.31        43\n",
            "DOCUMENTATION       0.67      0.29      0.40         7\n",
            "       PEOPLE       0.00      0.00      0.00         8\n",
            " REQUIREMENTS       0.50      0.34      0.40        62\n",
            "         TEST       0.92      0.81      0.86       151\n",
            "    USABILITY       0.62      0.26      0.37        19\n",
            "   VERSIONING       0.50      0.50      0.50         4\n",
            "\n",
            "     accuracy                           0.65       993\n",
            "    macro avg       0.55      0.40      0.45       993\n",
            " weighted avg       0.64      0.65      0.63       993\n",
            "\n",
            "==================================================\n",
            "Experimenting with TfidfVectorizer...\n",
            "Classification Report for TfidfVectorizer:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    ALGORITHM       0.50      0.04      0.08        47\n",
            " ARCHITECTURE       0.80      0.21      0.33        57\n",
            "        BUILD       1.00      0.34      0.51        29\n",
            "         CODE       0.56      0.93      0.70       428\n",
            "       DEFECT       0.53      0.41      0.47       138\n",
            "       DESIGN       0.44      0.09      0.15        43\n",
            "DOCUMENTATION       0.00      0.00      0.00         7\n",
            "       PEOPLE       0.00      0.00      0.00         8\n",
            " REQUIREMENTS       0.65      0.18      0.28        62\n",
            "         TEST       0.97      0.75      0.85       151\n",
            "    USABILITY       0.00      0.00      0.00        19\n",
            "   VERSIONING       0.67      0.50      0.57         4\n",
            "\n",
            "     accuracy                           0.61       993\n",
            "    macro avg       0.51      0.29      0.33       993\n",
            " weighted avg       0.62      0.61      0.56       993\n",
            "\n",
            "==================================================\n",
            "Experimenting with HashingVectorizer...\n",
            "Classification Report for HashingVectorizer:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    ALGORITHM       0.40      0.04      0.08        47\n",
            " ARCHITECTURE       0.82      0.16      0.26        57\n",
            "        BUILD       1.00      0.21      0.34        29\n",
            "         CODE       0.55      0.93      0.69       428\n",
            "       DEFECT       0.53      0.41      0.46       138\n",
            "       DESIGN       0.44      0.09      0.15        43\n",
            "DOCUMENTATION       0.00      0.00      0.00         7\n",
            "       PEOPLE       0.00      0.00      0.00         8\n",
            " REQUIREMENTS       0.69      0.18      0.28        62\n",
            "         TEST       0.97      0.75      0.85       151\n",
            "    USABILITY       0.00      0.00      0.00        19\n",
            "   VERSIONING       0.00      0.00      0.00         4\n",
            "\n",
            "     accuracy                           0.60       993\n",
            "    macro avg       0.45      0.23      0.26       993\n",
            " weighted avg       0.62      0.60      0.54       993\n",
            "\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install hmmlearn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTKDlMORQGDX",
        "outputId": "89cb062f-c5de-4130-e970-e48a712ad290"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hmmlearn\n",
            "  Downloading hmmlearn-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161 kB)\n",
            "\u001b[?25l     \u001b[90m\u001b[0m \u001b[32m0.0/161.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m\u001b[0m\u001b[90m\u001b[0m\u001b[90m\u001b[0m \u001b[32m153.6/161.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m\u001b[0m \u001b[32m161.1/161.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.10 in /usr/local/lib/python3.10/dist-packages (from hmmlearn) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn!=0.22.0,>=0.16 in /usr/local/lib/python3.10/dist-packages (from hmmlearn) (1.2.2)\n",
            "Requirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.10/dist-packages (from hmmlearn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn!=0.22.0,>=0.16->hmmlearn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn!=0.22.0,>=0.16->hmmlearn) (3.3.0)\n",
            "Installing collected packages: hmmlearn\n",
            "Successfully installed hmmlearn-0.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Donot run this cell : This code takes time to run and produces an error \"Mix of label input types string and number\" at the end."
      ],
      "metadata": {
        "id": "qwMSA7jimbMx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from hmmlearn import hmm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "data = vidoni\n",
        "\n",
        "# Feature Extraction\n",
        "vectorizer = TfidfVectorizer(max_features=1000)\n",
        "X = vectorizer.fit_transform(data['Comments'])\n",
        "y = data['TDType']\n",
        "\n",
        "# Step: Model Training\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "model = hmm.GaussianHMM(n_components=10, covariance_type=\"full\", n_iter=100)\n",
        "model.fit(X_train.toarray())\n",
        "\n",
        "# Step 5: Evaluation\n",
        "y_pred = model.predict(X_test.toarray())\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "CJ2yqJr2dgGu",
        "outputId": "2e2caa79-db46-4c9a-f6a4-fdec4e594bfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:hmmlearn.base:Fitting a model with 5015099 free scalar parameters with only 3968000 data points will result in a degenerate solution.\n",
            "WARNING:hmmlearn.base:Model is not converging.  Current: 19720082.612007115 is not greater than 19720082.612007346. Delta is -2.3096799850463867e-07\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Mix of label input types (string and number)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-73-c433e4f0015c>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Step 5: Evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[1;32m   2311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2312\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2313\u001b[0;31m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2314\u001b[0m         \u001b[0mlabels_given\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2315\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36munique_labels\u001b[0;34m(*ys)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;31m# Check that we don't mix string type with number type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mys_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Mix of label input types (string and number)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Mix of label input types (string and number)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MARKOV MODEL"
      ],
      "metadata": {
        "id": "Dqp4VbedmqiL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3zBm6IA4moBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# Load the dataset\n",
        "data = vidoni\n",
        "\n",
        "# Preprocess the data (tokenization)\n",
        "comments = data['Comments'].str.split()\n",
        "\n",
        "# Create the Markov model\n",
        "markov_model = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "# Train the model\n",
        "for comment in comments:\n",
        "    for i in range(len(comment) - 1):\n",
        "        word, next_word = comment[i], comment[i + 1]\n",
        "        markov_model[word][next_word] += 1\n",
        "\n",
        "# Normalize the counts to get transition probabilities\n",
        "for word in markov_model:\n",
        "    total_transitions = sum(markov_model[word].values())\n",
        "    for next_word in markov_model[word]:\n",
        "        markov_model[word][next_word] /= total_transitions\n",
        "\n",
        "# Example: Generate a sequence of words using the trained Markov model\n",
        "current_word = 'start'\n",
        "generated_sequence = [current_word]\n",
        "for _ in range(10):  # Generate 10 words\n",
        "    next_word = max(markov_model[current_word], key=markov_model[current_word].get)\n",
        "    generated_sequence.append(next_word)\n",
        "    current_word = next_word\n",
        "\n",
        "# Print the generated sequence\n",
        "print(' '.join(generated_sequence))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shreOlw1d4NB",
        "outputId": "5666e3ad-1389-4135-9e98-15e042936994"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start # TODO: add the same as well; if someone wanted\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IynHTRuxltdx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}