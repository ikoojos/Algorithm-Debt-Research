# -*- coding: utf-8 -*-
"""Final_CLS_Fine Tuned RoBERTa and CLS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Az_3LDIu56VoMmvC-TuOhcHmOIel7Ieo
"""

# Commented out IPython magic to ensure Python compatibility.
# %cd '/content/drive/My Drive/AD Final Experiments'

import sys
import os
from google.colab import drive
drive.mount('/content/drive')
sys.path.append('/content/drive/My Drive/AD Final Experiments')

import importlib
import numpy as np
import pandas as pd
from itertools import product

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler, MultiLabelBinarizer
from sklearn.multioutput import MultiOutputClassifier

from nltk.tokenize import word_tokenize
#from gensim.models import Word2Vec

import torch
from torch.utils.data import DataLoader, Dataset
from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback

from preprocessing import preprocess_data
from splitting import split_data
from utils import *
from evaluate_model import evaluate_best_model
from lr_tuning import hyperparameter_tuning

for module in ['preprocessing', 'splitting', 'utils', 'evaluate_model', 'lr_tuning']:
    importlib.reload(sys.modules[module])

file_path = '/content/drive/My Drive/AD Identification using SATD/liu_datset_processed.csv'
data = preprocess_data(file_path)

pip install transformers

pip install wandb

import wandb

wandb.init(mode="disabled")

class_mapping = {label: idx for idx, label in enumerate(data['TDType'].unique())}
data['label'] = data['TDType'].map(class_mapping)


X_train_temp, X_test, y_train_temp, y_test = train_test_split(data['Comments'], data['label'], test_size=0.2, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_train_temp, y_train_temp, test_size=0.2, random_state=42)

keywords = ['shape', 'input', 'tensor', 'number', 'matrix']

def extract_custom_features(texts):
    features = []
    for t in texts:
        t_lower = str(t).lower()
        features.append([int(kw in t_lower) for kw in keywords])
    return np.array(features)

custom_train = extract_custom_features(X_train)
custom_val   = extract_custom_features(X_val)
custom_test  = extract_custom_features(X_test)

# ===============================


# ===============================

# Define dataset class
# ===============================
class CustomADDataset(Dataset):
    def __init__(self, texts, labels, custom_features, tokenizer, max_length=128):
        self.texts = texts
        self.labels = labels
        self.custom_features = custom_features
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts.iloc[idx])
        label = torch.tensor(self.labels.iloc[idx], dtype=torch.long)
        custom_feat = torch.tensor(self.custom_features[idx], dtype=torch.float)

        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )

        return {
            'input_ids': encoding['input_ids'].squeeze(),
            'attention_mask': encoding['attention_mask'].squeeze(),
            'custom_features': custom_feat,
            'labels': label
        }

# ===============================
# Instantiate tokenizer & datasets
# ===============================
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')

train_dataset = CustomADDataset(X_train, y_train, custom_train, tokenizer)
val_dataset   = CustomADDataset(X_val, y_val, custom_val, tokenizer)
test_dataset  = CustomADDataset(X_test, y_test, custom_test, tokenizer)

from torch.optim import AdamW

import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import RobertaTokenizer, RobertaModel
# Define model with custom features
# ===============================
class RobertaWithCustomFeatures(nn.Module):
    def __init__(self, num_labels, num_custom_features, roberta_model_name='roberta-base', dropout=0.1):
        super(RobertaWithCustomFeatures, self).__init__()
        self.roberta = RobertaModel.from_pretrained(roberta_model_name)
        hidden_size = self.roberta.config.hidden_size
        self.dropout = nn.Dropout(dropout)
        self.classifier = nn.Linear(hidden_size + num_custom_features, num_labels)

    def forward(self, input_ids, attention_mask, custom_features, labels=None):
        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)
        cls_embedding = outputs.last_hidden_state[:, 0, :]  # [CLS] token
        combined = torch.cat((cls_embedding, custom_features), dim=1)
        combined = self.dropout(combined)
        logits = self.classifier(combined)

        if labels is not None:
            loss_fn = nn.CrossEntropyLoss()
            loss = loss_fn(logits, labels)
            return loss, logits
        return logits

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = RobertaWithCustomFeatures(num_labels=len(class_mapping), num_custom_features=len(keywords))
model.to(device)

train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
val_loader   = DataLoader(val_dataset, batch_size=16)
test_loader  = DataLoader(test_dataset, batch_size=16)

optimizer = AdamW(model.parameters(), lr=2e-5)
num_epochs = 10

# ===============================

print(device)   # should say "cuda"

model.to(device)

from tqdm import tqdm

for epoch in range(num_epochs):
    model.train()
    total_loss = 0
    for batch in tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs}"):
        optimizer.zero_grad()
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        custom_features = batch['custom_features'].to(device)
        labels = batch['labels'].to(device)

        loss, logits = model(input_ids, attention_mask, custom_features, labels)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    print(f"Epoch {epoch+1} - Loss: {total_loss/len(train_loader):.4f}")

# Evaluation

model.eval()
all_preds, all_labels = [], []

with torch.no_grad():
    for batch in test_loader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        custom_features = batch['custom_features'].to(device)
        labels = batch['labels'].to(device)

        logits = model(input_ids, attention_mask, custom_features)
        preds = torch.argmax(logits, dim=1)

        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

print("Classification Report:")

print(classification_report(all_labels, all_preds, target_names=class_mapping.keys()))

print("F1 Score (Weighted):", f1_score(all_labels, all_preds, average='weighted'))